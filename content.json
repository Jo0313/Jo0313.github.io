{"pages":[],"posts":[{"title":"20210323 내용","text":"setup, include1knitr::opts_chunk$set(echo = TRUE) 문제 1. 패키지 설치 방법 ggplot2와 dplyr 패키지 설치 방법을 기재하세요. 12library(ggplot2)library(dplyr) 문제 2. 질적 변수와 양적 변수의 차이에 대해 설명 하세요문제 3. 엑셀 데이터를 불러오세요.12counties &lt;- readxl::read_xlsx(&quot;counties.xslx&quot;, sheet = 1)glimpse(counties) 문제 4. private_work, unemployment를 활용하여 산점도를 작성하세요. 조건: region을 기준으로 그룹화를 진행합니다. 12ggplot(counties, aes(x = private_work, y = unemployment, colour = region)) + geom_point() 문제 5. dplyr 함수를 활용하여, 아래 데이터를 요약하세요. counties 데이터를 활용합니다. 변수 추출은 region, state, men, women 각 region, state 별 men, women 전체 인구수를 구합니다. 최종 데이터셋 저장 이름은 final_df로 명명합니다. 123456789counties %&gt;% select(region, state, county, men, women) %&gt;% group_by(region, state) %&gt;% summarize(total_men = sum(men), total_women = sum(women)) %&gt;% filter(region == &quot;North Central&quot;) %&gt;% arrange(desc(total_men)) -&gt; final_df glimpse(final_df) 문제 6. final_df를 기준으로 막대 그래프를 그립니다. x축: state 1개의 region만 선택 / 선택은 자유 색상 꾸미기 등은 자유입니다. 그래프 정렬도 안해도 됩니다. 12ggplot(final_df, aes(x = state, y = total_men)) + geom_col()","link":"/2021/03/23/20210323/"},{"title":"20210408.md","text":"","link":"/2021/04/08/20210408-md/"},{"title":"","text":"텍스트 마이닝 형태 - 수치형 - 이진형(y/n) - 범주형 - 텍스트 텍스트 마이닝? ㅁ 비정형화된 데이터 처리, 정형화하는 방법 ㅁ 블로그, 온라인 포럼, 리뷰사이트, 뉴스기사 , 트위터 활용사례 뉴스를 주제별로 정리 용어정리 말뭉치 : 관련된 문서들의 집합 문서 행렬 : 문서와 문서에 있는 단어를 행렬로 정리 파싱/토큰화 : 텍스트의 단어, 절을 분리하는 작업 용어(token) : 단어의 어간을 추출하는 과정 불용어 : 의미 없는 단어(the,and,of등)","link":"/2021/04/09/20210324/"},{"title":"","text":"분류 mnist 데이터를 활용 1.1 이진 분류기 훈련 sgdclassifier클래스 사용한 ‘확률적경사 하강법(SGD)’ 분류기 ㄴ 장점 : 큰 데이터 셋을 효율적으로 처리 sgdclassifier는 무작위성을 사용(이미지가 5인지 확인) 2.1 성능측정 ㅁ ‘교차 검증’을 사용한 정확도 측정 cross_val_score() 함수로 폴드가 3개인 ik-겹 교차 검증을 사용해 sgdclassifier모델 평가 시작 array값이 0.95 이상(95%이상) 더미 분류기 만들어서 확인 이 예제는 정확도를 분류기의성능 측정 지표로 선호하지 않는 이유를 보여줌, 어떤 클래스가 다른것보다 월등히 많은 경우 더욱 그렇다 ㅁ 오차행렬 분류기의 성능을 평가하는 더 좋은 방법은 오차 행렬을 조사하는 것이다 오차 행렬을 만들려면 실제 타깃과 비교할 수 있도록 예측값을 만들어야 한다. cross_val_predit() 함수를 쓰는데 평가 점수를 반환하지 않고 각 테스트 폴드에서 얻은 예측을 반환 confusion_matrix()함수를 사용해 타깃클래스, 예측클래스를 넣어 오차 행렬 만들기 행 : 실제 클래스 / 열 : 예측한 클래스 정밀도 = TP/TP+FP ( TP : 양성의 수, FP : 거짓 양성의 수) , 정밀도는 재현율과 같이 사용하는 것이 좋다. 재현율 = TP/TP+FN ㅁ 정밀도와 재현율 정밀도와 재현율을 F1점수라고 하는 하나의 숫자로 만들면 편리함 F1점수는 정밀도와 재현율의 조화 평균 F1점수를 구하려면 f1_score() 함수 호출 상황에 따라 정밀도가 중요할 수가 있고 재현율이 중요할 수 있음 (어린아이 유해사이트 지정) ㅁ 정밀도/재현율 트레이드오프 (임계값을 올리면) 정밀도가 올라가면 재현율이 줄어들고 (임곗값을 내리면)재현율이 올라가면 정밀도가 떨어진다 predict() 대신 decision_function()을 사용해 데이터 가져옴 , 이 점수 기반으로 임곗값을 정해 예측을 만들수 있다. 적절한 임곗값을 정하기 위해 cross_val_predit()을 사용해 모든 샘플의 점수 구하고 예측 결과가 아닌 결정 점수를 반환하도록 지정 precision_recall_curve()를 사용해 모든 임계값에 대해 정밀도와 재현율을 계산할 수 있다 맷플롯립을 이용해 임계값의 함수로 정밀도와 재현율을 그림 ㅁ ROC 곡선 이진 분류에서 널리 사용하는 도구 정밀도/재현율 곡선과 매우 비슷하지만, roc 곡선은 정밀도에 대한 재현율 곡선이 아니고 거짓 양성비율에대한 진짜 양성비율의 곡선 roc_curve()함수 사용해서 임계값에서 TPR과 FPR을 계산 좋은 분류기는 점선에서 멀리 떨어져 있어야한다. 곡선아래면적(AUC)를 측정하면 분류기들을 비교 가능 양성 클래스가 드물거나 거짓 음성보다 거짓양성이 더 중요할때 PR곡선을 사용하고 그렇지 않으면 ROC곡선을 사용 ※ 용어 거짓 양성비율 : FPR 진짜 양성비율 : TPR 진짜 음성비율 : TNR (true negative rate) 특이도 : TNR (specificity) 다중분류 둘 이상의 클래스를 구별 가능 ovo 주요 장점은 각 분류기의 훈련에 전체 훈련세트 중 구별할 두 클래스에 해당하는 샘플만 필요 일부 알고리즘은 큰 훈련 세트에서 몇개의 분류기를 훈련시키는것보다 작은 훈련세트에서 많은 분류기를 훈련시키는 쪽이 빠르므로 ovo를 선호 대부분의 이진 분류 알고리즘에서는 ovr을 선호 다중 클래스 분류 작업에 이진분류 알고리즘을 선택하면 사이킷런이 알고리즘에 따라 자동으로 ovo, ovr을 실행 사이킷런에서 ovo나 ovr을 사용하도록 강제하려면 OneVsOneClassifier나 OneVsRestClassifier를 사용 –&gt; 이진 분류기 인스턴스를 만들어 객체를 생성 할 때 전달하면 된다. decision_function 메소드는 클래스마다 하나의 값을 반환 cross_val_score()을 사용해 SGDClassifier의 정확도 평가 ※ 용어 Ova,OvR 전략 : 이미지를 분류 할 때 각 분류기의 결정 점수 중에서 가장 높은 것을 클래스로 선택 OvO 전략 : 1,2 1,3과 같이 각각의 숫자의 조합마다 이진 분류기를 훈련시키는 것 에러분석","link":"/2021/04/09/20210409/"},{"title":"R 정리","text":"dim()“매트릭스 또는 데이터 프레임의 행과 열의 개수를 출력”ex) dim(train) Show in New Window[1] 1460(행) 81(열) str(train[,c(1:10, 81)])“처음 10 개의 변수를 표시.” Show in New Window‘data.frame’: 1460 obs. of 3 variables:$ Id : int 1 2 3 4 5 6 7 8 9 10 …$ MSSubClass: int 60 20 60 70 60 50 20 60 50 190 …$ SalePrice : int 208500 181500 223500 140000 250000 143000 307000 200000 129900 118000 … test_labels &lt;- test$Id test.ID 값을 test_labels에 담음 rbind(v,v1)“두 행렬을 행방향으로 합침”ex)v = 6:15v1 = 1:5m1 = rbind(v,v1) Show in New Window [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]v 6 7 8 9 10 11 12 13 14 15v1 1 2 3 4 5 1 2 3 4 5 ggplot(data=all[!is.na(all$SalePrice),], aes(x=SalePrice)) + geom_histogram(fill=&quot;blue&quot;, binwidth = 10000) + scale_x_continuous(breaks= seq(0, 800000, by=100000), labels = comma) is.na() 변수 &quot;원소 중 NA(결측값)인 것을 True,False 로 출력, is this NA?라고 물어보는 것과 같다.&quot; ex) &gt; a&lt;-c(1,2,3,NA,4,5) a&lt;-c(1,2,3,NA,4,5)is.na(a)[1] FALSE FALSE FALSE TRUE FALSE FALSE a[is.na(a)]&lt;-0print(a)[1] 1 2 3 0 4 5 seq()“seq(from = 초기값,to = 최종값,by = 증감)”ex) seq(0, 800000, by=100000), labels = comma) 0에서 800000까지 100000 증가해서 나타내라 sapply()“sapply 함수는 apply 계열의 함수로서 R만의 특징이 반영된 함수입니다. 만약 어떤 기능을 반복적인 처리할 시 매우 편리한 함수입니다. 예를 들어, 데이터셋에 포함된 결측치를 0으로 대체한다든지 또는 열별로 평균이나 합계를 구하는 것 등 각 변수별로 적용할 필요없이 간단한 코드를 돌리는 것만으로도 전체 데이터셋에 반영됩니다. 변수가 두세 개 정도일 때는 별 문제가 안되지만 수십, 수백개일 때는 각 변수별로 처리하는 것은 굉장히 비효율적일 것입니다. R에서는 for문과 같이 전통적으로 쓰이는 반복문도 쓸 수 있지만 sapply 함수처럼 벡터별 처리를 하는 함수를 이용하면 더 효율적으로 코드를 구성할 수 있습니다.” which(x, arr.ind = FALSE)“특정 값의 위치를 찾을 수 있는 함수” ※ Err) ‘’ must only be used inside dplyr해결방법 &gt; dplyr:: 을 앞에 붙인다","link":"/2021/03/25/K-Proj/"},{"title":"Rbasic","text":"Data에는 질적 범주, 양적 범주가 있다 질적 범주 안에는 명목척도와 서열척도가 있음 ※ 명목척도(Nominal Scale): ㅁ 가장 낮은 수준의 척도로 단지 측정대상의 특성만 구분하기 위하여 숫자나 기호를 할당한 것으로 특성간의 양적인 분석을 할 수 없고, 때문에 특성간에 대소의 비료도 할 수 없다. ※ 서열척도(Ordinal Scale): ㅁ 측정대상의 특성들을 구분하여 줄뿐만 아니라 이들 사이의 상대적인 크기를 나타낼 수 있고, 서로 간에 비교가 가능한 척도 명목척도 닭고기 소고기 서열척도 닭고기 1등급 닭고기 2등급 연속형 discrete 사람과 관련된 데이터 고객 데이터 (crm) 아무리 분석을 해도 마케팅이 없으면 x 기계와 관련된 데이터 (무의미한 데이터가 많음) 제조공정과 관련된 데이터 ※ 0단계 : 패키지 설치 install.packages(“ggplot2”) ※ 1단계 : 패키지 불러오기 library(ggplot2) ※ 2단계 : Data 불러오기 data(“iris”) ※ 3 데이터확인 str(iris) ※ 4 가공되지 않은 데이터 가공하기 ( 시각화를 위해서) ※ 5 시각화 하기 ggplot(data=iris, mapping = aes(x = Petal.Length,y = Species)) + geom_point() ggplot(데이터, x축, y축) +그래프종류 + 옵션(예: 축의 범위, 그래프 제목, 그래프 색상, 그래프의 배경색)","link":"/2021/03/22/Rbasic/"},{"title":"c++","text":"#includeusing namespace std; int main(){ std &gt;&gt; “hello world”;}","link":"/2021/03/22/c/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2021/03/22/hello-world/"},{"title":"temp","text":"R 설치 R을 설치한다. URL: https://www.r-project.org/소스코드 작성 R 소스코드를 작성합니다. 1234a &lt;- 1b &lt;- 2c &lt;- a + bprint(c) 테스트 테스트","link":"/2021/03/22/temp/"},{"title":"study","text":"head (x)head는 데이터의 처음 6행의 값을 보여주는 함수입니다. dataframe을 입력하면 처음 6개의 행렬을, 데이터에 변수를 지정하여 입력하면 변수의 처음 6개의 값을 나타냅니다. head(diamonds) # 첫 행부터 6행까지의 값을 나타냄 A tibble: 6 x 10carat cut color clarity depth table price x y z 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.432 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.313 0.23 Good E VS1 56.9 65 327 4.05 4.07 2.314 0.290 Premium I VS2 62.4 58 334 4.2 4.23 2.635 0.31 GoodJ SI2 63.3 58 335 4.34 4.35 2.756 0.24 Very Good J VVS2 62.8 57 336 3.94 3.96 2.48 head(diamonds$carat) # carat 열의 6행까지의 값을 나타냄[1] 0.23 0.21 0.23 0.29 0.31 0.24 tail (x)tail은 마지막 6행의 값을 나타냅니다. head와 마찬가지로 행렬과 변수의 개별 값을 알 수 있습니다. tail(diamonds) A tibble: 6 x 10carat cut color clarity depth table price x y z 1 0.72 Premium D SI1 62.7 59 2757 5.69 5.73 3.582 0.72 Ideal D SI1 60.8 57 2757 5.75 5.76 3.53 0.72 Good D SI1 63.1 55 2757 5.69 5.75 3.614 0.7 Very Good D SI1 62.8 60 2757 5.66 5.68 3.565 0.86 Premium H SI2 61 58 2757 6.15 6.12 3.746 0.75 Ideal D SI2 62.2 55 2757 5.83 5.87 3.64 tail(diamonds$carat)[1] 0.72 0.72 0.72 0.70 0.86 0.75 str (x)str함수에 변수를 지정하면 해당 변수의 속성과 길이, 그리고 미리보기 값을 제공합니다. str(diamonds) 차례대로 데이터 구조(data.frame),데이터의 길이(53940 obs),변수의 개수(of 10 variables),변수 명($carat, $cut, … $z), 변수의 속성(num, ord.factor…),변수 값 미리보기 제공Classes ‘tbl_df’, ‘tbl’ and ‘data.frame’: 53940 obs. of 10 variables:$ carat : num 0.23 0.21 0.23 0.29 0.31 0.24 0.24 0.26 0.22 0.23 …$ cut : Ord.factor w/ 5 levels “Fair”&lt;”Good”&lt;..: 5 4 2 4 2 3 3 3 1 3 …$ color : Ord.factor w/ 7 levels “D”&lt;”E”&lt;”F”&lt;”G”&lt;..: 2 2 2 6 7 7 6 5 2 5 …$ clarity: Ord.factor w/ 8 levels “I1”&lt;”SI2”&lt;”SI1”&lt;..: 2 3 5 4 2 6 7 3 4 5 …$ depth : num 61.5 59.8 56.9 62.4 63.3 62.8 62.3 61.9 65.1 59.4 …$ table : num 55 61 65 58 58 57 57 55 61 61 …$ price : int 326 326 327 334 335 336 336 337 337 338 …$ x : num 3.95 3.89 4.05 4.2 4.34 3.94 3.95 4.07 3.87 4 …$ y : num 3.98 3.84 4.07 4.23 4.35 3.96 3.98 4.11 3.78 4.05 …$ z : num 2.43 2.31 2.31 2.63 2.75 2.48 2.47 2.53 2.49 2.39 … str(diamonds$carat) num [1:53940] 0.23 0.21 0.23 0.29 0.31 0.24 0.24 0.26 0.22 0.23 …4. class (x)class는 데이터의 속성만 표시합니다.class(diamonds)[1] “tbl_df” “tbl” “data.frame” class(diamonds$carat)[1] “numeric”모든 변수의 속성을 알고 싶다면 sapply를 사용하면 됩니다.sapply는 모든 변수에 같은 시행을 적용하라는 명령을 내립니다.즉, sapply(diamonds,class)는 class함수를 diamonds의 변수 전체에 반복하라는 뜻입니다.sapply(diamonds,class)$carat[1] “numeric” $cut[1] “ordered” “factor” $color[1] “ordered” “factor” $clarity[1] “ordered” “factor” $depth[1] “numeric” $table[1] “numeric” $price[1] “integer” $x[1] “numeric” $y[1] “numeric” $z[1] “numeric”","link":"/2021/03/23/study/"},{"title":"R Markdown","text":"R MarkdownThis is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com. When you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this: 1summary(cars) 1234567## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.:19.0 3rd Qu.: 56.00 ## Max. :25.0 Max. :120.00 Including PlotsYou can also embed plots, for example: Note that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot.","link":"/2021/03/22/temp2/"},{"title":"Tuple과 List 의 차이점","text":"튜플을 사용하는 이유는 리스트에 비해서 조금 더 공간 효율적이기 때문이다. 리스트는 요소를 추가하는 동작을 빠르게 수행할 수 있도록 더 많은 공간을 저장해둔다. 이 특징은 파이썬의 실용주의적 측면을 나타낸다. 이런 상황처럼 *args를 두고 리스트인지 튜플인지 언급하기 어려운 애매할 때는 그냥 상황을 쉽게 설명할 수 있도록 자료 구조(data structure)라는 표현을 쓴다. 둘 다 타입과 상관 없이 일련의 요소(element)를 갖을 수 있다. 두 타입 모두 요소의 순서를 관리한다. (세트(set)나 딕셔너리(dict)와 다르게 말이다.) my_list[1] = “two”my_list[1, ‘two’, 3]my_tuple[1] = “two”Traceback (most recent call last): File ““, line 1, in TypeError: ‘tuple’ object does not support item assignment 이 특징은 리스트와 튜플을 구분하는 유일한 기술적 차이점이지만 이 특징이 나타나는 부분은 여럿 존재한다. 예를 들면 리스트에는 .append() 메소드를 사용해서 새로운 요소를 추가할 수 있지만 튜플은 불가능하다. my_list.append(“four”)my_list[1, ‘two’, 3, ‘four’]my_tuple.append(“four”)Traceback (most recent call last): File ““, line 1, in AttributeError: ‘tuple’ object has no attribute ‘append’ 튜플은 .append() 메소드가 필요하지 않다. 튜플은 수정할 수 없기 때문이다.","link":"/2021/04/06/tuple/"},{"title":"","text":"12345from matplotlib.ticker import (MultipleLocator, AutoMinorLocator, FuncFormatter)def major_formatter(x, pos): return &quot;{%.2f}&quot; % xformatter = FuncFormatter(major_formatter) 12import numpy as npprint(np.__version__) 1.19.5 Numpy넘파이 shape는1개 (갯수,) 2개 이상 (행갯수,갯수) 1234data1 = [1,2,8],[3,4,5],[6,7,8]data1data2 =[1,2,3,4,5,6]data2 [1, 2, 3, 4, 5, 6] 123456array1 = np.array(data1)print(array1)print(array1.shape)array2 = np.array(data2)print(array2.shape) [[1 2 8] [3 4 5] [6 7 8]] (3, 3) (6,) 기본함수들12array = np.arange(1,8)array array([1, 2, 3, 4, 5, 6, 7]) 12zero = np.zeros((3,5))print(zero) [[0. 0. 0. 0. 0.] [0. 0. 0. 0. 0.] [0. 0. 0. 0. 0.]] 12one = np.ones((5,2))print(one) [[1. 1.] [1. 1.] [1. 1.] [1. 1.] [1. 1.]] #Pandas (영상, 이미지 등 비정형 데이터에서 주로 쓴다) 12import pandas as pdprint(pd.__version__) 1.1.5 ##구글 드라이브 연동 마운트하기 12from google.colab import drivedrive.mount('/content/drive') Mounted at /content/drive #데이터 처리 데이터 불러오기 &amp; 조회 123lemonade = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/kaggle2portpolio/data/Lemonade2016.csv')lemonade.info()lemonade.head(5) &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 32 entries, 0 to 31 Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Date 31 non-null object 1 Location 32 non-null object 2 Lemon 32 non-null int64 3 Orange 32 non-null int64 4 Temperature 32 non-null int64 5 Leaflets 31 non-null float64 6 Price 32 non-null float64 dtypes: float64(2), int64(3), object(2) memory usage: 1.9+ KB .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Date Location Lemon Orange Temperature Leaflets Price 0 7/1/2016 Park 97 67 70 90.0 0.25 1 7/2/2016 Park 98 67 72 90.0 0.25 2 7/3/2016 Park 110 77 71 104.0 0.25 3 7/4/2016 Beach 134 99 76 98.0 0.25 4 7/5/2016 Beach 159 118 78 135.0 0.25 요약 1lemonade.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Lemon Orange Temperature Leaflets Price count 32.000000 32.000000 32.000000 31.000000 32.000000 mean 116.156250 80.000000 78.968750 108.548387 0.354687 std 25.823357 21.863211 4.067847 20.117718 0.113137 min 71.000000 42.000000 70.000000 68.000000 0.250000 25% 98.000000 66.750000 77.000000 90.000000 0.250000 50% 113.500000 76.500000 80.500000 108.000000 0.350000 75% 131.750000 95.000000 82.000000 124.000000 0.500000 max 176.000000 129.000000 84.000000 158.000000 0.500000 값 가져오기 123lemonade['Location'] # 내용보기lemonade['Location'].value_counts() # 카운트 Beach 17 Park 15 Name: Location, dtype: int64 값 수정 12lemonade['Price'] = lemonade['Lemon'] + lemonade['Orange']lemonade.head(5) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Date Location Lemon Orange Temperature Leaflets Price 0 7/1/2016 Park 97 67 70 90.0 164 1 7/2/2016 Park 98 67 72 90.0 165 2 7/3/2016 Park 110 77 71 104.0 187 3 7/4/2016 Beach 134 99 76 98.0 233 4 7/5/2016 Beach 159 118 78 135.0 277 칼럼 추가 12lemonade['Test'] = lemonade['Price'] + lemonade['Leaflets']lemonade.head(5) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Date Location Lemon Orange Temperature Leaflets Price Test 0 7/1/2016 Park 97 67 70 90.0 164 254.0 1 7/2/2016 Park 98 67 72 90.0 165 255.0 2 7/3/2016 Park 110 77 71 104.0 187 291.0 3 7/4/2016 Beach 134 99 76 98.0 233 331.0 4 7/5/2016 Beach 159 118 78 135.0 277 412.0 칼럼 안보이게 출력 123lemonade_column_drop = lemonade.drop('Test',axis=1)#print(lemonade_column_drop.head(3))lemonade.head(3) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Date Location Lemon Orange Temperature Leaflets Price Test 0 7/1/2016 Park 97 67 70 90.0 164 254.0 1 7/2/2016 Park 98 67 72 90.0 165 255.0 2 7/3/2016 Park 110 77 71 104.0 187 291.0 칼럼크기 조절 12pd.set_option('display.max_columns', 8)print(lemonade.head(5)) Date Location Lemon Orange Temperature Leaflets Price Test 0 7/1/2016 Park 97 67 70 90.0 164 254.0 1 7/2/2016 Park 98 67 72 90.0 165 255.0 2 7/3/2016 Park 110 77 71 104.0 187 291.0 3 7/4/2016 Beach 134 99 76 98.0 233 331.0 4 7/5/2016 Beach 159 118 78 135.0 277 412.0 행 조절 1print(lemonade[0:5]) Date Location Lemon Orange Temperature Leaflets Price Test 0 7/1/2016 Park 97 67 70 90.0 164 254.0 1 7/2/2016 Park 98 67 72 90.0 165 255.0 2 7/3/2016 Park 110 77 71 104.0 187 291.0 3 7/4/2016 Beach 134 99 76 98.0 233 331.0 4 7/5/2016 Beach 159 118 78 135.0 277 412.0 조건 1print(lemonade[lemonade['Location'] == 'Beach'].head(3)) Date Location Lemon Orange Temperature Leaflets Price Test 3 7/4/2016 Beach 134 99 76 98.0 233 331.0 4 7/5/2016 Beach 159 118 78 135.0 277 412.0 5 7/6/2016 Beach 103 69 82 90.0 172 262.0 #파이썬 시각화 블로그 연습 1234567891011121314import matplotlib.pyplot as pltdates = [ '2021-01-01', '2021-01-02', '2021-01-03', '2021-01-04', '2021-01-05', '2021-01-06', '2021-01-07', '2021-01-08', '2021-01-09', '2021-01-10']min_temperature = [20.7, 17.9, 18.8, 14.6, 15.8, 15.8, 15.8, 17.4, 21.8, 20.0]max_temperature = [34.7, 28.9, 31.8, 25.6, 28.8, 21.8, 22.8, 28.4, 30.8, 32.0]#fig = 도화지 axes = 그래프 pl = pyplotfig,axes = plt.subplots(nrows=1, ncols=1, figsize=(10,6))axes.plot(dates, min_temperature, label = 'Min Temperature')axes.plot(dates, max_temperature, label = 'Max Temperature')axes.legend()plt.show() 12345678910111213141516import matplotlib.pyplot as pltimport numpy as npimport calendarmonth_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]sold_list = [300, 400, 550, 900, 600, 960, 900, 910, 800, 700, 550, 450]fig, ax = plt.subplots(figsize=(10,6))plot = ax.bar(month_list, sold_list)#plt.xticks(month_list, calendar.month_name[1:13], rotation=90)ax.set_xticks(month_list)#x축 ax.set_xticklabels(calendar.month_name[1:13], rotation=90) #x 축 라벨 지정for rect in plot: print(rect) height = rect.get_height() ax.text(rect.get_x() + rect.get_width()/2., 1.002*height,'%d' % int(height), ha='center', va='bottom')plt.show() Rectangle(xy=(0.6, 0), width=0.8, height=300, angle=0) Rectangle(xy=(1.6, 0), width=0.8, height=400, angle=0) Rectangle(xy=(2.6, 0), width=0.8, height=550, angle=0) Rectangle(xy=(3.6, 0), width=0.8, height=900, angle=0) Rectangle(xy=(4.6, 0), width=0.8, height=600, angle=0) Rectangle(xy=(5.6, 0), width=0.8, height=960, angle=0) Rectangle(xy=(6.6, 0), width=0.8, height=900, angle=0) Rectangle(xy=(7.6, 0), width=0.8, height=910, angle=0) Rectangle(xy=(8.6, 0), width=0.8, height=800, angle=0) Rectangle(xy=(9.6, 0), width=0.8, height=700, angle=0) Rectangle(xy=(10.6, 0), width=0.8, height=550, angle=0) Rectangle(xy=(11.6, 0), width=0.8, height=450, angle=0) #페가블로그 코드 1234import matplotlib.pyplot as pltfrom matplotlib.ticker import (MultipleLocator, AutoMinorLocator, FuncFormatter)import seaborn as snsimport numpy as np 123456789101112131415161718192021222324252627def plot_example(ax, zorder=0): ax.bar(tips_day[&quot;day&quot;], tips_day[&quot;tip&quot;], color=&quot;lightgray&quot;, zorder=zorder) ax.set_title(&quot;tip (mean)&quot;, fontsize=16, pad=12) # Values h_pad = 0.1 for i in range(4): fontweight = &quot;normal&quot; color = &quot;k&quot; if i == 3: fontweight = &quot;bold&quot; color = &quot;darkred&quot; ax.text(i, tips_day[&quot;tip&quot;].loc[i] + h_pad, f&quot;{tips_day['tip'].loc[i]:0.2f}&quot;, horizontalalignment='center', fontsize=12, fontweight=fontweight, color=color) # Sunday ax.patches[3].set_facecolor(&quot;darkred&quot;) ax.patches[3].set_edgecolor(&quot;black&quot;) # set_range ax.set_ylim(0, 4) return axdef major_formatter(x, pos): return &quot;{%.2f}&quot; % xformatter = FuncFormatter(major_formatter) 123tips = sns.load_dataset(&quot;tips&quot;)tips_day = tips.groupby(&quot;day&quot;).mean().reset_index()print(tips_day) day total_bill tip size 0 Thur 17.682742 2.771452 2.451613 1 Fri 17.151579 2.734737 2.105263 2 Sat 20.441379 2.993103 2.517241 3 Sun 21.410000 3.255132 2.842105 12fig, ax = plt.subplots(figsize=(10, 6))ax = plot_example(ax, zorder=2) 123456fig, ax = plt.subplots(figsize=(10, 6))ax = plot_example(ax, zorder=2)ax.spines[&quot;top&quot;].set_visible(False)ax.spines[&quot;right&quot;].set_visible(False)ax.spines[&quot;left&quot;].set_visible(False) 12345678910fig, ax = plt.subplots()ax = plot_example(ax, zorder=2)ax.spines[&quot;top&quot;].set_visible(False)ax.spines[&quot;right&quot;].set_visible(False)ax.spines[&quot;left&quot;].set_visible(False)ax.yaxis.set_major_locator(MultipleLocator(1))ax.yaxis.set_major_formatter(formatter)ax.yaxis.set_minor_locator(MultipleLocator(0.5)) 12345678910111213fig, ax = plt.subplots()ax = plot_example(ax, zorder=2)ax.spines[&quot;top&quot;].set_visible(False)ax.spines[&quot;right&quot;].set_visible(False)ax.spines[&quot;left&quot;].set_visible(False)ax.yaxis.set_major_locator(MultipleLocator(1))ax.yaxis.set_major_formatter(formatter)ax.yaxis.set_minor_locator(MultipleLocator(0.5)) ax.grid(axis=&quot;y&quot;, which=&quot;major&quot;, color=&quot;lightgray&quot;)ax.grid(axis=&quot;y&quot;, which=&quot;minor&quot;, ls=&quot;:&quot;) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556import matplotlib.pyplot as pltimport seaborn as snsimport numpy as nptips = sns.load_dataset(&quot;tips&quot;)fig, ax = plt.subplots(nrows = 1, ncols = 2, figsize=(16, 5))# Ideal Bar Graphax0 = sns.barplot(x = &quot;day&quot;, y = 'total_bill', data = tips, ci=None, color='lightgray', alpha=0.85, zorder=2, ax=ax[0])group_mean = tips.groupby(['day'])['total_bill'].agg('mean')h_day = group_mean.sort_values(ascending=False).index[0]h_mean = np.round(group_mean.sort_values(ascending=False)[0], 2)for p in ax0.patches: fontweight = &quot;normal&quot; color = &quot;k&quot; height = np.round(p.get_height(), 2) if h_mean == height: fontweight=&quot;bold&quot; color=&quot;darkred&quot; p.set_facecolor(color) p.set_edgecolor(&quot;black&quot;) ax0.text(p.get_x() + p.get_width()/2., height+1, height, ha = 'center', size=12, fontweight=fontweight, color=color)ax0.set_ylim(-3, 30)ax0.set_title(&quot;Ideal Bar Graph&quot;, size = 16)ax0.spines['top'].set_visible(False)ax0.spines['left'].set_position((&quot;outward&quot;, 20))ax0.spines['left'].set_visible(False)ax0.spines['right'].set_visible(False)ax0.set_xlabel(&quot;Weekday&quot;, fontsize=14)for xtick in ax0.get_xticklabels(): if xtick.get_text() == h_day: xtick.set_color(&quot;darkred&quot;) xtick.set_fontweight(&quot;demibold&quot;)ax0.set_xticklabels(['Thursday', 'Friday', 'Saturday', 'Sunday'], size=12)ax0.set_ylabel(&quot;Avg. Total Bill($)&quot;, fontsize=14)ax0.grid(axis=&quot;y&quot;, which=&quot;major&quot;, color=&quot;lightgray&quot;)ax0.grid(axis=&quot;y&quot;, which=&quot;minor&quot;, ls=&quot;:&quot;)ax1 = sns.barplot(x = &quot;day&quot;, y = 'total_bill', data = tips, ci=None, alpha=0.85, ax=ax[1])for p in ax1.patches: height = np.round(p.get_height(), 2) ax1.text(p.get_x() + p.get_width()/2., height+1, height, ha = 'center', size=12)ax1.set_ylim(-3, 30)ax1.set_title(&quot;Just Bar Graph&quot;)fig.show() 1","link":"/2021/04/07/Untitled0/"},{"title":"","text":"로지스틱 회귀 h1 {font-size: 34px;} h1.title {font-size: 38px;} h2 {font-size: 30px;} h3 {font-size: 24px;} h4 {font-size: 18px;} h5 {font-size: 16px;} h6 {font-size: 12px;} code {color: inherit; background-color: rgba(0, 0, 0, 0.04);} pre:not([class]) { background-color: white } code{white-space: pre-wrap;} span.smallcaps{font-variant: small-caps;} span.underline{text-decoration: underline;} div.column{display: inline-block; vertical-align: top; width: 50%;} div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;} ul.task-list{list-style: none;} code{white-space: pre;} if (window.hljs) { hljs.configure({languages: []}); hljs.initHighlightingOnLoad(); if (document.readyState && document.readyState === \"complete\") { window.setTimeout(function() { hljs.initHighlighting(); }, 0); } } .main-container { max-width: 940px; margin-left: auto; margin-right: auto; } img { max-width:100%; } .tabbed-pane { padding-top: 12px; } .html-widget { margin-bottom: 20px; } button.code-folding-btn:focus { outline: none; } summary { display: list-item; } pre code { padding: 0; } .tabset-dropdown > .nav-tabs { display: inline-table; max-height: 500px; min-height: 44px; overflow-y: auto; border: 1px solid #ddd; border-radius: 4px; } .tabset-dropdown > .nav-tabs > li.active:before { content: \"\"; font-family: 'Glyphicons Halflings'; display: inline-block; padding: 10px; border-right: 1px solid #ddd; } .tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before { content: \"\"; border: none; } .tabset-dropdown > .nav-tabs.nav-tabs-open:before { content: \"\"; font-family: 'Glyphicons Halflings'; display: inline-block; padding: 10px; border-right: 1px solid #ddd; } .tabset-dropdown > .nav-tabs > li.active { display: block; } .tabset-dropdown > .nav-tabs > li > a, .tabset-dropdown > .nav-tabs > li > a:focus, .tabset-dropdown > .nav-tabs > li > a:hover { border: none; display: inline-block; border-radius: 4px; background-color: transparent; } .tabset-dropdown > .nav-tabs.nav-tabs-open > li { display: block; float: none; } .tabset-dropdown > .nav-tabs > li { display: none; } 로지스틱 회귀 2021 3 31 library(ggplot2) library(cowplot) ## NOTE: The data used in this demo comes from the UCI machine learning ## repository. ## http://archive.ics.uci.edu/ml/index.php ## Specifically, this is the heart disease data set. ## http://archive.ics.uci.edu/ml/datasets/Heart+Disease url &lt;- &quot;http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data&quot; data &lt;- read.csv(url, header=FALSE) head(data) ## V1 V2 V3 V4 V5 V6 V7 V8 V9 V10 V11 V12 V13 V14 ## 1 63 1 1 145 233 1 2 150 0 2.3 3 0.0 6.0 0 ## 2 67 1 4 160 286 0 2 108 1 1.5 2 3.0 3.0 2 ## 3 67 1 4 120 229 0 2 129 1 2.6 2 2.0 7.0 1 ## 4 37 1 3 130 250 0 0 187 0 3.5 3 0.0 3.0 0 ## 5 41 0 2 130 204 0 2 172 0 1.4 1 0.0 3.0 0 ## 6 56 1 2 120 236 0 0 178 0 0.8 1 0.0 3.0 0 colnames(data) &lt;- c( &quot;age&quot;, &quot;sex&quot;,# 0 = female, 1 = male &quot;cp&quot;, # chest pain # 1 = typical angina, # 2 = atypical angina, # 3 = non-anginal pain, # 4 = asymptomatic &quot;trestbps&quot;, # resting blood pressure (in mm Hg) &quot;chol&quot;, # serum cholestoral in mg/dl &quot;fbs&quot;, # fasting blood sugar if less than 120 mg/dl, 1 = TRUE, 0 = FALSE &quot;restecg&quot;, # resting electrocardiographic results # 1 = normal # 2 = having ST-T wave abnormality # 3 = showing probable or definite left ventricular hypertrophy &quot;thalach&quot;, # maximum heart rate achieved &quot;exang&quot;, # exercise induced angina, 1 = yes, 0 = no &quot;oldpeak&quot;, # ST depression induced by exercise relative to rest &quot;slope&quot;, # the slope of the peak exercise ST segment # 1 = upsloping # 2 = flat # 3 = downsloping &quot;ca&quot;, # number of major vessels (0-3) colored by fluoroscopy &quot;thal&quot;, # this is short of thalium heart scan # 3 = normal (no cold spots) # 6 = fixed defect (cold spots during rest and exercise) # 7 = reversible defect (when cold spots only appear during exercise) &quot;hd&quot; # (the predicted attribute) - diagnosis of heart disease # 0 if less than or equal to 50% diameter narrowing # 1 if greater than 50% diameter narrowing ) head(data) # now we have data and column names ## age sex cp trestbps chol fbs restecg thalach exang oldpeak slope ca thal hd ## 1 63 1 1 145 233 1 2 150 0 2.3 3 0.0 6.0 0 ## 2 67 1 4 160 286 0 2 108 1 1.5 2 3.0 3.0 2 ## 3 67 1 4 120 229 0 2 129 1 2.6 2 2.0 7.0 1 ## 4 37 1 3 130 250 0 0 187 0 3.5 3 0.0 3.0 0 ## 5 41 0 2 130 204 0 2 172 0 1.4 1 0.0 3.0 0 ## 6 56 1 2 120 236 0 0 178 0 0.8 1 0.0 3.0 0 str(data) # this shows that we need to tell R which columns contain factors ## 'data.frame': 303 obs. of 14 variables: ## $ age : num 63 67 67 37 41 56 62 57 63 53 ... ## $ sex : num 1 1 1 1 0 1 0 0 1 1 ... ## $ cp : num 1 4 4 3 2 2 4 4 4 4 ... ## $ trestbps: num 145 160 120 130 130 120 140 120 130 140 ... ## $ chol : num 233 286 229 250 204 236 268 354 254 203 ... ## $ fbs : num 1 0 0 0 0 0 0 0 0 1 ... ## $ restecg : num 2 2 2 0 2 0 2 0 2 2 ... ## $ thalach : num 150 108 129 187 172 178 160 163 147 155 ... ## $ exang : num 0 1 1 0 0 0 0 1 0 1 ... ## $ oldpeak : num 2.3 1.5 2.6 3.5 1.4 0.8 3.6 0.6 1.4 3.1 ... ## $ slope : num 3 2 2 3 1 1 3 1 2 3 ... ## $ ca : chr &quot;0.0&quot; &quot;3.0&quot; &quot;2.0&quot; &quot;0.0&quot; ... ## $ thal : chr &quot;6.0&quot; &quot;3.0&quot; &quot;7.0&quot; &quot;3.0&quot; ... ## $ hd : int 0 2 1 0 0 0 3 0 2 1 ... # it also shows us that there are some missing values. There are &quot;?&quot;s # in the dataset. These are in the &quot;ca&quot; and &quot;thal&quot; columns... ## First, convert &quot;?&quot;s to NAs... data[data == &quot;?&quot;] &lt;- NA ## Now add factors for variables that are factors and clean up the factors ## that had missing data... data[data$sex == 0,]$sex &lt;- &quot;F&quot; data[data$sex == 1,]$sex &lt;- &quot;M&quot; data$sex &lt;- as.factor(data$sex) data$cp &lt;- as.factor(data$cp) data$fbs &lt;- as.factor(data$fbs) data$restecg &lt;- as.factor(data$restecg) data$exang &lt;- as.factor(data$exang) data$slope &lt;- as.factor(data$slope) data$ca &lt;- as.integer(data$ca) # since this column had &quot;?&quot;s in it # R thinks that the levels for the factor are strings, but # we know they are integers, so first convert the strings to integers... data$ca &lt;- as.factor(data$ca) # ...then convert the integers to factor levels data$thal &lt;- as.integer(data$thal) # &quot;thal&quot; also had &quot;?&quot;s in it. data$thal &lt;- as.factor(data$thal) ## This next line replaces 0 and 1 with &quot;Healthy&quot; and &quot;Unhealthy&quot; data$hd &lt;- ifelse(test=data$hd == 0, yes=&quot;Healthy&quot;, no=&quot;Unhealthy&quot;) data$hd &lt;- as.factor(data$hd) # Now convert to a factor str(data) ## this shows that the correct columns are factors ## 'data.frame': 303 obs. of 14 variables: ## $ age : num 63 67 67 37 41 56 62 57 63 53 ... ## $ sex : Factor w/ 2 levels &quot;F&quot;,&quot;M&quot;: 2 2 2 2 1 2 1 1 2 2 ... ## $ cp : Factor w/ 4 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;: 1 4 4 3 2 2 4 4 4 4 ... ## $ trestbps: num 145 160 120 130 130 120 140 120 130 140 ... ## $ chol : num 233 286 229 250 204 236 268 354 254 203 ... ## $ fbs : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 2 1 1 1 1 1 1 1 1 2 ... ## $ restecg : Factor w/ 3 levels &quot;0&quot;,&quot;1&quot;,&quot;2&quot;: 3 3 3 1 3 1 3 1 3 3 ... ## $ thalach : num 150 108 129 187 172 178 160 163 147 155 ... ## $ exang : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 1 2 2 1 1 1 1 2 1 2 ... ## $ oldpeak : num 2.3 1.5 2.6 3.5 1.4 0.8 3.6 0.6 1.4 3.1 ... ## $ slope : Factor w/ 3 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;: 3 2 2 3 1 1 3 1 2 3 ... ## $ ca : Factor w/ 4 levels &quot;0&quot;,&quot;1&quot;,&quot;2&quot;,&quot;3&quot;: 1 4 3 1 1 1 3 1 2 1 ... ## $ thal : Factor w/ 3 levels &quot;3&quot;,&quot;6&quot;,&quot;7&quot;: 2 1 3 1 1 1 1 1 3 3 ... ## $ hd : Factor w/ 2 levels &quot;Healthy&quot;,&quot;Unhealthy&quot;: 1 2 2 1 1 1 2 1 2 2 ... ## Now determine how many rows have &quot;NA&quot; (aka &quot;Missing data&quot;). If it's just ## a few, we can remove them from the dataset, otherwise we should consider ## imputing the values with a Random Forest or some other imputation method. nrow(data[is.na(data$ca) | is.na(data$thal),]) ## [1] 6 data[is.na(data$ca) | is.na(data$thal),] ## age sex cp trestbps chol fbs restecg thalach exang oldpeak slope ca thal ## 88 53 F 3 128 216 0 2 115 0 0.0 1 0 &lt;NA&gt; ## 167 52 M 3 138 223 0 0 169 0 0.0 1 &lt;NA&gt; 3 ## 193 43 M 4 132 247 1 2 143 1 0.1 2 &lt;NA&gt; 7 ## 267 52 M 4 128 204 1 0 156 1 1.0 2 0 &lt;NA&gt; ## 288 58 M 2 125 220 0 0 144 0 0.4 2 &lt;NA&gt; 7 ## 303 38 M 3 138 175 0 0 173 0 0.0 1 &lt;NA&gt; 3 ## hd ## 88 Healthy ## 167 Healthy ## 193 Unhealthy ## 267 Unhealthy ## 288 Healthy ## 303 Healthy ## so 6 of the 303 rows of data have missing values. This isn't a large ## percentage (2%), so we can just remove them from the dataset ## NOTE: This is different from when we did machine learning with ## Random Forests. When we did that, we imputed values. nrow(data) ## [1] 303 data &lt;- data[!(is.na(data$ca) | is.na(data$thal)),] nrow(data) ## [1] 297 ##################################### ## ## Now we can do some quality control by making sure all of the factor ## levels are represented by people with and without heart disease (hd) ## ## NOTE: We also want to exclude variables that only have 1 or 2 samples in ## a category since +/- one or two samples can have a large effect on the ## odds/log(odds) ## ## ##################################### xtabs(~ hd + sex, data=data) ## sex ## hd F M ## Healthy 71 89 ## Unhealthy 25 112 xtabs(~ hd + cp, data=data) ## cp ## hd 1 2 3 4 ## Healthy 16 40 65 39 ## Unhealthy 7 9 18 103 xtabs(~ hd + fbs, data=data) ## fbs ## hd 0 1 ## Healthy 137 23 ## Unhealthy 117 20 xtabs(~ hd + restecg, data=data) ## restecg ## hd 0 1 2 ## Healthy 92 1 67 ## Unhealthy 55 3 79 xtabs(~ hd + exang, data=data) ## exang ## hd 0 1 ## Healthy 137 23 ## Unhealthy 63 74 xtabs(~ hd + slope, data=data) ## slope ## hd 1 2 3 ## Healthy 103 48 9 ## Unhealthy 36 89 12 xtabs(~ hd + ca, data=data) ## ca ## hd 0 1 2 3 ## Healthy 129 21 7 3 ## Unhealthy 45 44 31 17 xtabs(~ hd + thal, data=data) ## thal ## hd 3 6 7 ## Healthy 127 6 27 ## Unhealthy 37 12 88 ##################################### ## ## Now we are ready for some logistic regression. First we'll create a very ## simple model that uses sex to predict heart disease ## ##################################### ## let's start super simple and see if sex (female/male) is a good ## predictor... ## First, let's just look at the raw data... xtabs(~ hd + sex, data=data) ## sex ## hd F M ## Healthy 71 89 ## Unhealthy 25 112 # sex # hd F M # Healthy 71 89 # Unhealthy 25 112 ## Most of the females are healthy and most of the males are unhealthy. ## Being female is likely to decrease the odds in being unhealthy. ## In other words, if a sample is female, the odds are against it that it ## will be unhealthy ## Being male is likely to increase the odds in being unhealthy... ## In other words, if a sample is male, the odds are for it being unhealthy ########### ## ## Now do the actual logistic regression ## ########### logistic &lt;- glm(hd ~ sex, data=data, family=&quot;binomial&quot;) #glm함수에 일반화된 선형모델을 먼저 수행하는 함수, hd ~ sex :성별을 사용하여 심장질환 예측 지정정 ,data=data : 모델에사용될 데이터지정, family=&quot;binomial&quot; : 일반화 된 선형의 이항패밀리 #logistic &lt;- glm(hd ~ sex, data=data, family=&quot;binomial&quot;) 여기 물결표는 심장 질환 HD를 모델링 한다는 의미 summary(logistic) ## ## Call: ## glm(formula = hd ~ sex, family = &quot;binomial&quot;, data = data) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.2765 -1.2765 -0.7768 1.0815 1.6404 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.0438 0.2326 -4.488 7.18e-06 *** ## sexM 1.2737 0.2725 4.674 2.95e-06 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 409.95 on 296 degrees of freedom ## Residual deviance: 386.12 on 295 degrees of freedom ## AIC: 390.12 ## ## Number of Fisher Scoring iterations: 4 ## (Intercept) -1.0438 0.2326 -4.488 7.18e-06 *** ## sexM 1.2737 0.2725 4.674 2.95e-06 *** ## Let's start by going through the first coefficient... ## (Intercept) -1.0438 0.2326 -4.488 7.18e-06 *** ## ## The intercept is the log(odds) a female will be unhealthy. This is because ## female is the first factor in &quot;sex&quot; (the factors are ordered, ## alphabetically by default,&quot;female&quot;, &quot;male&quot;) female.log.odds &lt;- log(25 / 71) female.log.odds ## [1] -1.043804 ## Now let's look at the second coefficient... ## sexM 1.2737 0.2725 4.674 2.95e-06 *** ## ## sexM is the log(odds ratio) that tells us that if a sample has sex=M, the ## odds of being unhealthy are, on a log scale, 1.27 times greater than if ## a sample has sex=F. male.log.odds.ratio &lt;- log((112 / 89) / (25/71)) male.log.odds.ratio ## [1] 1.273667 ## Now calculate the overall &quot;Pseudo R-squared&quot; and its p-value ## NOTE: Since we are doing logistic regression... ## Null devaince = 2*(0 - LogLikelihood(null model)) ## = -2*LogLikihood(null model) ## Residual deviacne = 2*(0 - LogLikelihood(proposed model)) ## = -2*LogLikelihood(proposed model) ll.null &lt;- logistic$null.deviance/-2 ll.proposed &lt;- logistic$deviance/-2 ## McFadden's Pseudo R^2 = [ LL(Null) - LL(Proposed) ] / LL(Null) (ll.null - ll.proposed) / ll.null ## [1] 0.05812569 ## chi-square value = 2*(LL(Proposed) - LL(Null)) ## p-value = 1 - pchisq(chi-square value, df = 2-1) 1 - pchisq(2*(ll.proposed - ll.null), df=1) ## [1] 1.053157e-06 1 - pchisq((logistic$null.deviance - logistic$deviance), df=1) ## [1] 1.053157e-06 ## Lastly, let's see what this logistic regression predicts, given ## that a patient is either female or male (and no other data about them). predicted.data &lt;- data.frame( probability.of.hd=logistic$fitted.values, sex=data$sex) ## We can plot the data... ggplot(data=predicted.data, aes(x=sex, y=probability.of.hd)) + geom_point(aes(color=sex), size=5) + xlab(&quot;Sex&quot;) + ylab(&quot;Predicted probability of getting heart disease&quot;) ## Since there are only two probabilities (one for females and one for males), ## we can use a table to summarize the predicted probabilities. xtabs(~ probability.of.hd + sex, data=predicted.data) ## sex ## probability.of.hd F M ## 0.260416666667241 96 0 ## 0.55721393034826 0 201 ##################################### ## ## Now we will use all of the data available to predict heart disease ## ##################################### logistic &lt;- glm(hd ~ ., data=data, family=&quot;binomial&quot;) summary(logistic) ## ## Call: ## glm(formula = hd ~ ., family = &quot;binomial&quot;, data = data) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -3.0490 -0.4847 -0.1213 0.3039 2.9086 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -6.253978 2.960399 -2.113 0.034640 * ## age -0.023508 0.025122 -0.936 0.349402 ## sexM 1.670152 0.552486 3.023 0.002503 ** ## cp2 1.448396 0.809136 1.790 0.073446 . ## cp3 0.393353 0.700338 0.562 0.574347 ## cp4 2.373287 0.709094 3.347 0.000817 *** ## trestbps 0.027720 0.011748 2.359 0.018300 * ## chol 0.004445 0.004091 1.087 0.277253 ## fbs1 -0.574079 0.592539 -0.969 0.332622 ## restecg1 1.000887 2.638393 0.379 0.704424 ## restecg2 0.486408 0.396327 1.227 0.219713 ## thalach -0.019695 0.011717 -1.681 0.092781 . ## exang1 0.653306 0.447445 1.460 0.144267 ## oldpeak 0.390679 0.239173 1.633 0.102373 ## slope2 1.302289 0.486197 2.679 0.007395 ** ## slope3 0.606760 0.939324 0.646 0.518309 ## ca1 2.237444 0.514770 4.346 1.38e-05 *** ## ca2 3.271852 0.785123 4.167 3.08e-05 *** ## ca3 2.188715 0.928644 2.357 0.018428 * ## thal6 -0.168439 0.810310 -0.208 0.835331 ## thal7 1.433319 0.440567 3.253 0.001141 ** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 409.95 on 296 degrees of freedom ## Residual deviance: 183.10 on 276 degrees of freedom ## AIC: 225.1 ## ## Number of Fisher Scoring iterations: 6 ## Now calculate the overall &quot;Pseudo R-squared&quot; and its p-value ll.null &lt;- logistic$null.deviance/-2 ll.proposed &lt;- logistic$deviance/-2 ## McFadden's Pseudo R^2 = [ LL(Null) - LL(Proposed) ] / LL(Null) (ll.null - ll.proposed) / ll.null ## [1] 0.5533531 ## The p-value for the R^2 1 - pchisq(2*(ll.proposed - ll.null), df=(length(logistic$coefficients)-1)) ## [1] 0 ## now we can plot the data predicted.data &lt;- data.frame( probability.of.hd=logistic$fitted.values, hd=data$hd) predicted.data &lt;- predicted.data[ order(predicted.data$probability.of.hd, decreasing=FALSE),] predicted.data$rank &lt;- 1:nrow(predicted.data) ## Lastly, we can plot the predicted probabilities for each sample having ## heart disease and color by whether or not they actually had heart disease ggplot(data=predicted.data, aes(x=rank, y=probability.of.hd)) + geom_point(aes(color=hd), alpha=1, shape=4, stroke=2) + xlab(&quot;Index&quot;) + ylab(&quot;Predicted probability of getting heart disease&quot;) library(mlogit) ## Loading required package: dfidx ## ## Attaching package: 'dfidx' ## The following object is masked from 'package:stats': ## ## filter data(&quot;Fishing&quot;, package = &quot;mlogit&quot;) Fish &lt;- dfidx(Fishing, varying = 2:9, shape = &quot;wide&quot;, choice = &quot;mode&quot;) ## a pure &quot;conditional&quot; model summary(mlogit(mode ~ price + catch, data = Fish)) ## ## Call: ## mlogit(formula = mode ~ price + catch, data = Fish, method = &quot;nr&quot;) ## ## Frequencies of alternatives:choice ## beach boat charter pier ## 0.11337 0.35364 0.38240 0.15059 ## ## nr method ## 7 iterations, 0h:0m:0s ## g'(-H)^-1g = 6.22E-06 ## successive function values within tolerance limits ## ## Coefficients : ## Estimate Std. Error z-value Pr(&gt;|z|) ## (Intercept):boat 0.8713749 0.1140428 7.6408 2.154e-14 *** ## (Intercept):charter 1.4988884 0.1329328 11.2755 &lt; 2.2e-16 *** ## (Intercept):pier 0.3070552 0.1145738 2.6800 0.0073627 ** ## price -0.0247896 0.0017044 -14.5444 &lt; 2.2e-16 *** ## catch 0.3771689 0.1099707 3.4297 0.0006042 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Log-Likelihood: -1230.8 ## McFadden R^2: 0.17823 ## Likelihood ratio test : chisq = 533.88 (p.value = &lt; 2.22e-16) // add bootstrap table styles to pandoc tables function bootstrapStylePandocTables() { $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed'); } $(document).ready(function () { bootstrapStylePandocTables(); }); $(document).ready(function () { window.buildTabsets(\"TOC\"); }); $(document).ready(function () { $('.tabset-dropdown > .nav-tabs > li').click(function () { $(this).parent().toggleClass('nav-tabs-open'); }); }); (function () { var script = document.createElement(\"script\"); script.type = \"text/javascript\"; script.src = \"https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\"; document.getElementsByTagName(\"head\")[0].appendChild(script); })();","link":"/2021/04/06/Logistic-regression2/"},{"title":"","text":"20210323 내용 h1 {font-size: 34px;} h1.title {font-size: 38px;} h2 {font-size: 30px;} h3 {font-size: 24px;} h4 {font-size: 18px;} h5 {font-size: 16px;} h6 {font-size: 12px;} code {color: inherit; background-color: rgba(0, 0, 0, 0.04);} pre:not([class]) { background-color: white } code{white-space: pre-wrap;} span.smallcaps{font-variant: small-caps;} span.underline{text-decoration: underline;} div.column{display: inline-block; vertical-align: top; width: 50%;} div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;} ul.task-list{list-style: none;} code{white-space: pre;} if (window.hljs) { hljs.configure({languages: []}); hljs.initHighlightingOnLoad(); if (document.readyState && document.readyState === \"complete\") { window.setTimeout(function() { hljs.initHighlighting(); }, 0); } } .main-container { max-width: 940px; margin-left: auto; margin-right: auto; } img { max-width:100%; } .tabbed-pane { padding-top: 12px; } .html-widget { margin-bottom: 20px; } button.code-folding-btn:focus { outline: none; } summary { display: list-item; } pre code { padding: 0; } .tabset-dropdown > .nav-tabs { display: inline-table; max-height: 500px; min-height: 44px; overflow-y: auto; border: 1px solid #ddd; border-radius: 4px; } .tabset-dropdown > .nav-tabs > li.active:before { content: \"\"; font-family: 'Glyphicons Halflings'; display: inline-block; padding: 10px; border-right: 1px solid #ddd; } .tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before { content: \"\"; border: none; } .tabset-dropdown > .nav-tabs.nav-tabs-open:before { content: \"\"; font-family: 'Glyphicons Halflings'; display: inline-block; padding: 10px; border-right: 1px solid #ddd; } .tabset-dropdown > .nav-tabs > li.active { display: block; } .tabset-dropdown > .nav-tabs > li > a, .tabset-dropdown > .nav-tabs > li > a:focus, .tabset-dropdown > .nav-tabs > li > a:hover { border: none; display: inline-block; border-radius: 4px; background-color: transparent; } .tabset-dropdown > .nav-tabs.nav-tabs-open > li { display: block; float: none; } .tabset-dropdown > .nav-tabs > li { display: none; } 20210323 내용 조심재 2021 3 23 문제 1. 패키지 설치 방법 ggplot2와 dplyr 패키지 설치 방법을 기재하세요. library(ggplot2) library(dplyr) ## ## Attaching package: 'dplyr' ## The following objects are masked from 'package:stats': ## ## filter, lag ## The following objects are masked from 'package:base': ## ## intersect, setdiff, setequal, union 문제 2. 질적 변수와 양적 변수의 차이에 대해 설명 하세요 문제 3. 엑셀 데이터를 불러오세요. counties &lt;- readxl::read_xlsx(&quot;counties.xslx&quot;, sheet = 1) glimpse(counties) ## Rows: 3,138 ## Columns: 40 ## $ census_id &lt;chr&gt; &quot;1001&quot;, &quot;1003&quot;, &quot;1005&quot;, &quot;1007&quot;, &quot;1009&quot;, &quot;1011&quot;, &quot;10~ ## $ state &lt;chr&gt; &quot;Alabama&quot;, &quot;Alabama&quot;, &quot;Alabama&quot;, &quot;Alabama&quot;, &quot;Alabam~ ## $ county &lt;chr&gt; &quot;Autauga&quot;, &quot;Baldwin&quot;, &quot;Barbour&quot;, &quot;Bibb&quot;, &quot;Blount&quot;, ~ ## $ region &lt;chr&gt; &quot;South&quot;, &quot;South&quot;, &quot;South&quot;, &quot;South&quot;, &quot;South&quot;, &quot;South~ ## $ metro &lt;chr&gt; &quot;Metro&quot;, &quot;Metro&quot;, &quot;Nonmetro&quot;, &quot;Metro&quot;, &quot;Metro&quot;, &quot;No~ ## $ population &lt;dbl&gt; 55221, 195121, 26932, 22604, 57710, 10678, 20354, 1~ ## $ men &lt;dbl&gt; 26745, 95314, 14497, 12073, 28512, 5660, 9502, 5627~ ## $ women &lt;dbl&gt; 28476, 99807, 12435, 10531, 29198, 5018, 10852, 603~ ## $ hispanic &lt;dbl&gt; 2.6, 4.5, 4.6, 2.2, 8.6, 4.4, 1.2, 3.5, 0.4, 1.5, 7~ ## $ white &lt;dbl&gt; 75.8, 83.1, 46.2, 74.5, 87.9, 22.2, 53.3, 73.0, 57.~ ## $ black &lt;dbl&gt; 18.5, 9.5, 46.7, 21.4, 1.5, 70.7, 43.8, 20.3, 40.3,~ ## $ native &lt;dbl&gt; 0.4, 0.6, 0.2, 0.4, 0.3, 1.2, 0.1, 0.2, 0.2, 0.6, 0~ ## $ asian &lt;dbl&gt; 1.0, 0.7, 0.4, 0.1, 0.1, 0.2, 0.4, 0.9, 0.8, 0.3, 0~ ## $ pacific &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0~ ## $ citizens &lt;dbl&gt; 40725, 147695, 20714, 17495, 42345, 8057, 15581, 88~ ## $ income &lt;dbl&gt; 51281, 50254, 32964, 38678, 45813, 31938, 32229, 41~ ## $ income_err &lt;dbl&gt; 2391, 1263, 2973, 3995, 3141, 5884, 1793, 925, 2949~ ## $ income_per_cap &lt;dbl&gt; 24974, 27317, 16824, 18431, 20532, 17580, 18390, 21~ ## $ income_per_cap_err &lt;dbl&gt; 1080, 711, 798, 1618, 708, 2055, 714, 489, 1366, 15~ ## $ poverty &lt;dbl&gt; 12.9, 13.4, 26.7, 16.8, 16.7, 24.6, 25.4, 20.5, 21.~ ## $ child_poverty &lt;dbl&gt; 18.6, 19.2, 45.3, 27.9, 27.2, 38.4, 39.2, 31.6, 37.~ ## $ professional &lt;dbl&gt; 33.2, 33.1, 26.8, 21.5, 28.5, 18.8, 27.5, 27.3, 23.~ ## $ service &lt;dbl&gt; 17.0, 17.7, 16.1, 17.9, 14.1, 15.0, 16.6, 17.7, 14.~ ## $ office &lt;dbl&gt; 24.2, 27.1, 23.1, 17.8, 23.9, 19.7, 21.9, 24.2, 26.~ ## $ construction &lt;dbl&gt; 8.6, 10.8, 10.8, 19.0, 13.5, 20.1, 10.3, 10.5, 11.5~ ## $ production &lt;dbl&gt; 17.1, 11.2, 23.1, 23.7, 19.9, 26.4, 23.7, 20.4, 24.~ ## $ drive &lt;dbl&gt; 87.5, 84.7, 83.8, 83.2, 84.9, 74.9, 84.5, 85.3, 85.~ ## $ carpool &lt;dbl&gt; 8.8, 8.8, 10.9, 13.5, 11.2, 14.9, 12.4, 9.4, 11.9, ~ ## $ transit &lt;dbl&gt; 0.1, 0.1, 0.4, 0.5, 0.4, 0.7, 0.0, 0.2, 0.2, 0.2, 0~ ## $ walk &lt;dbl&gt; 0.5, 1.0, 1.8, 0.6, 0.9, 5.0, 0.8, 1.2, 0.3, 0.6, 1~ ## $ other_transp &lt;dbl&gt; 1.3, 1.4, 1.5, 1.5, 0.4, 1.7, 0.6, 1.2, 0.4, 0.7, 1~ ## $ work_at_home &lt;dbl&gt; 1.8, 3.9, 1.6, 0.7, 2.3, 2.8, 1.7, 2.7, 2.1, 2.5, 1~ ## $ mean_commute &lt;dbl&gt; 26.5, 26.4, 24.1, 28.8, 34.9, 27.5, 24.6, 24.1, 25.~ ## $ employed &lt;dbl&gt; 23986, 85953, 8597, 8294, 22189, 3865, 7813, 47401,~ ## $ private_work &lt;dbl&gt; 73.6, 81.5, 71.8, 76.8, 82.0, 79.5, 77.4, 74.1, 85.~ ## $ public_work &lt;dbl&gt; 20.9, 12.3, 20.8, 16.1, 13.5, 15.1, 16.2, 20.8, 12.~ ## $ self_employed &lt;dbl&gt; 5.5, 5.8, 7.3, 6.7, 4.2, 5.4, 6.2, 5.0, 2.8, 7.9, 4~ ## $ family_work &lt;dbl&gt; 0.0, 0.4, 0.1, 0.4, 0.4, 0.0, 0.2, 0.1, 0.0, 0.5, 0~ ## $ unemployment &lt;dbl&gt; 7.6, 7.5, 17.6, 8.3, 7.7, 18.0, 10.9, 12.3, 8.9, 7.~ ## $ land_area &lt;dbl&gt; 594.44, 1589.78, 884.88, 622.58, 644.78, 622.81, 77~ 문제 4. private_work, unemployment를 활용하여 산점도를 작성하세요. 조건: region을 기준으로 그룹화를 진행합니다. ggplot(counties, aes(x = private_work, y = unemployment, colour = region)) + geom_point() 문제 5. dplyr 함수를 활용하여, 아래 데이터를 요약하세요. counties 데이터를 활용합니다. 변수 추출은 region, state, men, women 각 region, state 별 men, women 전체 인구수를 구합니다. 최종 데이터셋 저장 이름은 final_df로 명명합니다. counties %&gt;% select(region, state, county, men, women) %&gt;% group_by(region, state) %&gt;% summarize(total_men = sum(men), total_women = sum(women)) %&gt;% filter(region == &quot;North Central&quot;) %&gt;% arrange(desc(total_men)) -&gt; final_df ## `summarise()` has grouped output by 'region'. You can override using the `.groups` argument. glimpse(final_df) ## Rows: 12 ## Columns: 4 ## Groups: region [1] ## $ region &lt;chr&gt; &quot;North Central&quot;, &quot;North Central&quot;, &quot;North Central&quot;, &quot;North ~ ## $ state &lt;chr&gt; &quot;Illinois&quot;, &quot;Ohio&quot;, &quot;Michigan&quot;, &quot;Indiana&quot;, &quot;Missouri&quot;, &quot;Wi~ ## $ total_men &lt;dbl&gt; 6316899, 5662893, 4861973, 3235263, 2964003, 2851385, 2692~ ## $ total_women &lt;dbl&gt; 6556862, 5913084, 5038598, 3333382, 3081445, 2890732, 2727~ 문제 6. final_df를 기준으로 막대 그래프를 그립니다. x축: state 1개의 region만 선택 / 선택은 자유 색상 꾸미기 등은 자유입니다. 그래프 정렬도 안해도 됩니다. ggplot(final_df, aes(x = state, y = total_men)) + geom_col() // add bootstrap table styles to pandoc tables function bootstrapStylePandocTables() { $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed'); } $(document).ready(function () { bootstrapStylePandocTables(); }); $(document).ready(function () { window.buildTabsets(\"TOC\"); }); $(document).ready(function () { $('.tabset-dropdown > .nav-tabs > li').click(function () { $(this).parent().toggleClass('nav-tabs-open'); }); }); (function () { var script = document.createElement(\"script\"); script.type = \"text/javascript\"; script.src = \"https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\"; document.getElementsByTagName(\"head\")[0].appendChild(script); })();","link":"/2021/03/23/20210323/"}],"tags":[{"name":"python","slug":"python","link":"/tags/python/"},{"name":"R","slug":"R","link":"/tags/R/"},{"name":"c++","slug":"c","link":"/tags/c/"},{"name":"Python","slug":"Python","link":"/tags/Python/"}],"categories":[]}