{"pages":[],"posts":[{"title":"20210323 내용","text":"setup, include1knitr::opts_chunk$set(echo = TRUE) 문제 1. 패키지 설치 방법 ggplot2와 dplyr 패키지 설치 방법을 기재하세요. 12library(ggplot2)library(dplyr) 문제 2. 질적 변수와 양적 변수의 차이에 대해 설명 하세요문제 3. 엑셀 데이터를 불러오세요.12counties &lt;- readxl::read_xlsx(&quot;counties.xslx&quot;, sheet = 1)glimpse(counties) 문제 4. private_work, unemployment를 활용하여 산점도를 작성하세요. 조건: region을 기준으로 그룹화를 진행합니다. 12ggplot(counties, aes(x = private_work, y = unemployment, colour = region)) + geom_point() 문제 5. dplyr 함수를 활용하여, 아래 데이터를 요약하세요. counties 데이터를 활용합니다. 변수 추출은 region, state, men, women 각 region, state 별 men, women 전체 인구수를 구합니다. 최종 데이터셋 저장 이름은 final_df로 명명합니다. 123456789counties %&gt;% select(region, state, county, men, women) %&gt;% group_by(region, state) %&gt;% summarize(total_men = sum(men), total_women = sum(women)) %&gt;% filter(region == &quot;North Central&quot;) %&gt;% arrange(desc(total_men)) -&gt; final_df glimpse(final_df) 문제 6. final_df를 기준으로 막대 그래프를 그립니다. x축: state 1개의 region만 선택 / 선택은 자유 색상 꾸미기 등은 자유입니다. 그래프 정렬도 안해도 됩니다. 12ggplot(final_df, aes(x = state, y = total_men)) + geom_col()","link":"/2021/03/23/20210323/"},{"title":"","text":"텍스트 마이닝 형태 - 수치형 - 이진형(y/n) - 범주형 - 텍스트 텍스트 마이닝? ㅁ 비정형화된 데이터 처리, 정형화하는 방법 ㅁ 블로그, 온라인 포럼, 리뷰사이트, 뉴스기사 , 트위터 활용사례 뉴스를 주제별로 정리 용어정리 말뭉치 : 관련된 문서들의 집합 문서 행렬 : 문서와 문서에 있는 단어를 행렬로 정리 파싱/토큰화 : 텍스트의 단어, 절을 분리하는 작업 용어(token) : 단어의 어간을 추출하는 과정 불용어 : 의미 없는 단어(the,and,of등)","link":"/2021/04/09/20210324/"},{"title":"20210408.md","text":"","link":"/2021/04/08/20210408-md/"},{"title":"R 정리","text":"dim()“매트릭스 또는 데이터 프레임의 행과 열의 개수를 출력”ex) dim(train) Show in New Window[1] 1460(행) 81(열) str(train[,c(1:10, 81)])“처음 10 개의 변수를 표시.” Show in New Window‘data.frame’: 1460 obs. of 3 variables:$ Id : int 1 2 3 4 5 6 7 8 9 10 …$ MSSubClass: int 60 20 60 70 60 50 20 60 50 190 …$ SalePrice : int 208500 181500 223500 140000 250000 143000 307000 200000 129900 118000 … test_labels &lt;- test$Id test.ID 값을 test_labels에 담음 rbind(v,v1)“두 행렬을 행방향으로 합침”ex)v = 6:15v1 = 1:5m1 = rbind(v,v1) Show in New Window [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]v 6 7 8 9 10 11 12 13 14 15v1 1 2 3 4 5 1 2 3 4 5 ggplot(data=all[!is.na(all$SalePrice),], aes(x=SalePrice)) + geom_histogram(fill=&quot;blue&quot;, binwidth = 10000) + scale_x_continuous(breaks= seq(0, 800000, by=100000), labels = comma) is.na() 변수 &quot;원소 중 NA(결측값)인 것을 True,False 로 출력, is this NA?라고 물어보는 것과 같다.&quot; ex) &gt; a&lt;-c(1,2,3,NA,4,5) a&lt;-c(1,2,3,NA,4,5)is.na(a)[1] FALSE FALSE FALSE TRUE FALSE FALSE a[is.na(a)]&lt;-0print(a)[1] 1 2 3 0 4 5 seq()“seq(from = 초기값,to = 최종값,by = 증감)”ex) seq(0, 800000, by=100000), labels = comma) 0에서 800000까지 100000 증가해서 나타내라 sapply()“sapply 함수는 apply 계열의 함수로서 R만의 특징이 반영된 함수입니다. 만약 어떤 기능을 반복적인 처리할 시 매우 편리한 함수입니다. 예를 들어, 데이터셋에 포함된 결측치를 0으로 대체한다든지 또는 열별로 평균이나 합계를 구하는 것 등 각 변수별로 적용할 필요없이 간단한 코드를 돌리는 것만으로도 전체 데이터셋에 반영됩니다. 변수가 두세 개 정도일 때는 별 문제가 안되지만 수십, 수백개일 때는 각 변수별로 처리하는 것은 굉장히 비효율적일 것입니다. R에서는 for문과 같이 전통적으로 쓰이는 반복문도 쓸 수 있지만 sapply 함수처럼 벡터별 처리를 하는 함수를 이용하면 더 효율적으로 코드를 구성할 수 있습니다.” which(x, arr.ind = FALSE)“특정 값의 위치를 찾을 수 있는 함수” ※ Err) ‘’ must only be used inside dplyr해결방법 &gt; dplyr:: 을 앞에 붙인다","link":"/2021/03/25/K-Proj/"},{"title":"","text":"분류 mnist 데이터를 활용 1.1 이진 분류기 훈련 sgdclassifier클래스 사용한 ‘확률적경사 하강법(SGD)’ 분류기 ㄴ 장점 : 큰 데이터 셋을 효율적으로 처리 sgdclassifier는 무작위성을 사용(이미지가 5인지 확인) 2.1 성능측정 ㅁ ‘교차 검증’을 사용한 정확도 측정 cross_val_score() 함수로 폴드가 3개인 ik-겹 교차 검증을 사용해 sgdclassifier모델 평가 시작 array값이 0.95 이상(95%이상) 더미 분류기 만들어서 확인 이 예제는 정확도를 분류기의성능 측정 지표로 선호하지 않는 이유를 보여줌, 어떤 클래스가 다른것보다 월등히 많은 경우 더욱 그렇다 ㅁ 오차행렬 분류기의 성능을 평가하는 더 좋은 방법은 오차 행렬을 조사하는 것이다 오차 행렬을 만들려면 실제 타깃과 비교할 수 있도록 예측값을 만들어야 한다. cross_val_predit() 함수를 쓰는데 평가 점수를 반환하지 않고 각 테스트 폴드에서 얻은 예측을 반환 confusion_matrix()함수를 사용해 타깃클래스, 예측클래스를 넣어 오차 행렬 만들기 행 : 실제 클래스 / 열 : 예측한 클래스 정밀도 = TP/TP+FP , 정밀도는 재현율과 같이 사용하는 것이 좋다. 재현율 = TP/TP+FN ※ 용어 FN : 거짓 음성의 수 TP : 양성의 수 FP : 거짓 양성의 수 ㅁ 정밀도와 재현율 정밀도와 재현율을 F1점수라고 하는 하나의 숫자로 만들면 편리함 F1점수는 정밀도와 재현율의 조화 평균 F1점수를 구하려면 f1_score() 함수 호출 상황에 따라 정밀도가 중요할 수가 있고 재현율이 중요할 수 있음 (어린아이 유해사이트 지정) ㅁ 정밀도/재현율 트레이드오프 (임계값을 올리면) 정밀도가 올라가면 재현율이 줄어들고 (임곗값을 내리면)재현율이 올라가면 정밀도가 떨어진다 predict() 대신 decision_function()을 사용해 데이터 가져옴 , 이 점수 기반으로 임곗값을 정해 예측을 만들수 있다. 적절한 임곗값을 정하기 위해 cross_val_predit()을 사용해 모든 샘플의 점수 구하고 예측 결과가 아닌 결정 점수를 반환하도록 지정 precision_recall_curve()를 사용해 모든 임계값에 대해 정밀도와 재현율을 계산할 수 있다 맷플롯립을 이용해 임계값의 함수로 정밀도와 재현율을 그림 ㅁ ROC 곡선 이진 분류에서 널리 사용하는 도구 정밀도/재현율 곡선과 매우 비슷하지만, roc 곡선은 정밀도에 대한 재현율 곡선이 아니고 거짓 양성비율에대한 진짜 양성비율의 곡선 roc_curve()함수 사용해서 임계값에서 TPR과 FPR을 계산 좋은 분류기는 점선에서 멀리 떨어져 있어야한다. 곡선아래면적(AUC)를 측정하면 분류기들을 비교 가능 양성 클래스가 드물거나 거짓 음성보다 거짓양성이 더 중요할때 PR곡선을 사용하고 그렇지 않으면 ROC곡선을 사용 ※ 용어 거짓 양성비율 : FPR 진짜 양성비율 : TPR 진짜 음성비율 : TNR (true negative rate) 특이도 : TNR (specificity) 다중분류 둘 이상의 클래스를 구별 가능 ovo 주요 장점은 각 분류기의 훈련에 전체 훈련세트 중 구별할 두 클래스에 해당하는 샘플만 필요 일부 알고리즘은 큰 훈련 세트에서 몇개의 분류기를 훈련시키는것보다 작은 훈련세트에서 많은 분류기를 훈련시키는 쪽이 빠르므로 ovo를 선호 대부분의 이진 분류 알고리즘에서는 ovr을 선호 다중 클래스 분류 작업에 이진분류 알고리즘을 선택하면 사이킷런이 알고리즘에 따라 자동으로 ovo, ovr을 실행 사이킷런에서 ovo나 ovr을 사용하도록 강제하려면 OneVsOneClassifier나 OneVsRestClassifier를 사용 –&gt; 이진 분류기 인스턴스를 만들어 객체를 생성 할 때 전달하면 된다. decision_function 메소드는 클래스마다 하나의 값을 반환 cross_val_score()을 사용해 SGDClassifier의 정확도 평가 ※ 용어 Ova,OvR 전략 : 이미지를 분류 할 때 각 분류기의 결정 점수 중에서 가장 높은 것을 클래스로 선택 OvO 전략 : 1,2 1,3과 같이 각각의 숫자의 조합마다 이진 분류기를 훈련시키는 것 에러분석 다중 레이블 분류 다중 레이블 분류에서 한 레이블이 다중 클래스가 될 수 있도록 일반화한 것(값을 두개 이상 가질 수 있음)","link":"/2021/04/09/20210409/"},{"title":"Rbasic","text":"Data에는 질적 범주, 양적 범주가 있다 질적 범주 안에는 명목척도와 서열척도가 있음 ※ 명목척도(Nominal Scale): ㅁ 가장 낮은 수준의 척도로 단지 측정대상의 특성만 구분하기 위하여 숫자나 기호를 할당한 것으로 특성간의 양적인 분석을 할 수 없고, 때문에 특성간에 대소의 비료도 할 수 없다. ※ 서열척도(Ordinal Scale): ㅁ 측정대상의 특성들을 구분하여 줄뿐만 아니라 이들 사이의 상대적인 크기를 나타낼 수 있고, 서로 간에 비교가 가능한 척도 명목척도 닭고기 소고기 서열척도 닭고기 1등급 닭고기 2등급 연속형 discrete 사람과 관련된 데이터 고객 데이터 (crm) 아무리 분석을 해도 마케팅이 없으면 x 기계와 관련된 데이터 (무의미한 데이터가 많음) 제조공정과 관련된 데이터 ※ 0단계 : 패키지 설치 install.packages(“ggplot2”) ※ 1단계 : 패키지 불러오기 library(ggplot2) ※ 2단계 : Data 불러오기 data(“iris”) ※ 3 데이터확인 str(iris) ※ 4 가공되지 않은 데이터 가공하기 ( 시각화를 위해서) ※ 5 시각화 하기 ggplot(data=iris, mapping = aes(x = Petal.Length,y = Species)) + geom_point() ggplot(데이터, x축, y축) +그래프종류 + 옵션(예: 축의 범위, 그래프 제목, 그래프 색상, 그래프의 배경색)","link":"/2021/03/22/Rbasic/"},{"title":"c++","text":"#includeusing namespace std; int main(){ std &gt;&gt; “hello world”;}","link":"/2021/03/22/c/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2021/03/22/hello-world/"},{"title":"temp","text":"R 설치 R을 설치한다. URL: https://www.r-project.org/소스코드 작성 R 소스코드를 작성합니다. 1234a &lt;- 1b &lt;- 2c &lt;- a + bprint(c) 테스트 테스트","link":"/2021/03/22/temp/"},{"title":"study","text":"head (x)head는 데이터의 처음 6행의 값을 보여주는 함수입니다. dataframe을 입력하면 처음 6개의 행렬을, 데이터에 변수를 지정하여 입력하면 변수의 처음 6개의 값을 나타냅니다. head(diamonds) # 첫 행부터 6행까지의 값을 나타냄 A tibble: 6 x 10carat cut color clarity depth table price x y z 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.432 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.313 0.23 Good E VS1 56.9 65 327 4.05 4.07 2.314 0.290 Premium I VS2 62.4 58 334 4.2 4.23 2.635 0.31 GoodJ SI2 63.3 58 335 4.34 4.35 2.756 0.24 Very Good J VVS2 62.8 57 336 3.94 3.96 2.48 head(diamonds$carat) # carat 열의 6행까지의 값을 나타냄[1] 0.23 0.21 0.23 0.29 0.31 0.24 tail (x)tail은 마지막 6행의 값을 나타냅니다. head와 마찬가지로 행렬과 변수의 개별 값을 알 수 있습니다. tail(diamonds) A tibble: 6 x 10carat cut color clarity depth table price x y z 1 0.72 Premium D SI1 62.7 59 2757 5.69 5.73 3.582 0.72 Ideal D SI1 60.8 57 2757 5.75 5.76 3.53 0.72 Good D SI1 63.1 55 2757 5.69 5.75 3.614 0.7 Very Good D SI1 62.8 60 2757 5.66 5.68 3.565 0.86 Premium H SI2 61 58 2757 6.15 6.12 3.746 0.75 Ideal D SI2 62.2 55 2757 5.83 5.87 3.64 tail(diamonds$carat)[1] 0.72 0.72 0.72 0.70 0.86 0.75 str (x)str함수에 변수를 지정하면 해당 변수의 속성과 길이, 그리고 미리보기 값을 제공합니다. str(diamonds) 차례대로 데이터 구조(data.frame),데이터의 길이(53940 obs),변수의 개수(of 10 variables),변수 명($carat, $cut, … $z), 변수의 속성(num, ord.factor…),변수 값 미리보기 제공Classes ‘tbl_df’, ‘tbl’ and ‘data.frame’: 53940 obs. of 10 variables:$ carat : num 0.23 0.21 0.23 0.29 0.31 0.24 0.24 0.26 0.22 0.23 …$ cut : Ord.factor w/ 5 levels “Fair”&lt;”Good”&lt;..: 5 4 2 4 2 3 3 3 1 3 …$ color : Ord.factor w/ 7 levels “D”&lt;”E”&lt;”F”&lt;”G”&lt;..: 2 2 2 6 7 7 6 5 2 5 …$ clarity: Ord.factor w/ 8 levels “I1”&lt;”SI2”&lt;”SI1”&lt;..: 2 3 5 4 2 6 7 3 4 5 …$ depth : num 61.5 59.8 56.9 62.4 63.3 62.8 62.3 61.9 65.1 59.4 …$ table : num 55 61 65 58 58 57 57 55 61 61 …$ price : int 326 326 327 334 335 336 336 337 337 338 …$ x : num 3.95 3.89 4.05 4.2 4.34 3.94 3.95 4.07 3.87 4 …$ y : num 3.98 3.84 4.07 4.23 4.35 3.96 3.98 4.11 3.78 4.05 …$ z : num 2.43 2.31 2.31 2.63 2.75 2.48 2.47 2.53 2.49 2.39 … str(diamonds$carat) num [1:53940] 0.23 0.21 0.23 0.29 0.31 0.24 0.24 0.26 0.22 0.23 …4. class (x)class는 데이터의 속성만 표시합니다.class(diamonds)[1] “tbl_df” “tbl” “data.frame” class(diamonds$carat)[1] “numeric”모든 변수의 속성을 알고 싶다면 sapply를 사용하면 됩니다.sapply는 모든 변수에 같은 시행을 적용하라는 명령을 내립니다.즉, sapply(diamonds,class)는 class함수를 diamonds의 변수 전체에 반복하라는 뜻입니다.sapply(diamonds,class)$carat[1] “numeric” $cut[1] “ordered” “factor” $color[1] “ordered” “factor” $clarity[1] “ordered” “factor” $depth[1] “numeric” $table[1] “numeric” $price[1] “integer” $x[1] “numeric” $y[1] “numeric” $z[1] “numeric”","link":"/2021/03/23/study/"},{"title":"R Markdown","text":"R MarkdownThis is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com. When you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this: 1summary(cars) 1234567## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.:19.0 3rd Qu.: 56.00 ## Max. :25.0 Max. :120.00 Including PlotsYou can also embed plots, for example: Note that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot.","link":"/2021/03/22/temp2/"},{"title":"Tuple과 List 의 차이점","text":"튜플을 사용하는 이유는 리스트에 비해서 조금 더 공간 효율적이기 때문이다. 리스트는 요소를 추가하는 동작을 빠르게 수행할 수 있도록 더 많은 공간을 저장해둔다. 이 특징은 파이썬의 실용주의적 측면을 나타낸다. 이런 상황처럼 *args를 두고 리스트인지 튜플인지 언급하기 어려운 애매할 때는 그냥 상황을 쉽게 설명할 수 있도록 자료 구조(data structure)라는 표현을 쓴다. 둘 다 타입과 상관 없이 일련의 요소(element)를 갖을 수 있다. 두 타입 모두 요소의 순서를 관리한다. (세트(set)나 딕셔너리(dict)와 다르게 말이다.) my_list[1] = “two”my_list[1, ‘two’, 3]my_tuple[1] = “two”Traceback (most recent call last): File ““, line 1, in TypeError: ‘tuple’ object does not support item assignment 이 특징은 리스트와 튜플을 구분하는 유일한 기술적 차이점이지만 이 특징이 나타나는 부분은 여럿 존재한다. 예를 들면 리스트에는 .append() 메소드를 사용해서 새로운 요소를 추가할 수 있지만 튜플은 불가능하다. my_list.append(“four”)my_list[1, ‘two’, 3, ‘four’]my_tuple.append(“four”)Traceback (most recent call last): File ““, line 1, in AttributeError: ‘tuple’ object has no attribute ‘append’ 튜플은 .append() 메소드가 필요하지 않다. 튜플은 수정할 수 없기 때문이다.","link":"/2021/04/06/tuple/"},{"title":"","text":"12345from matplotlib.ticker import (MultipleLocator, AutoMinorLocator, FuncFormatter)def major_formatter(x, pos): return &quot;{%.2f}&quot; % xformatter = FuncFormatter(major_formatter) 12import numpy as npprint(np.__version__) 1.19.5 Numpy넘파이 shape는1개 (갯수,) 2개 이상 (행갯수,갯수) 1234data1 = [1,2,8],[3,4,5],[6,7,8]data1data2 =[1,2,3,4,5,6]data2 [1, 2, 3, 4, 5, 6] 123456array1 = np.array(data1)print(array1)print(array1.shape)array2 = np.array(data2)print(array2.shape) [[1 2 8] [3 4 5] [6 7 8]] (3, 3) (6,) 기본함수들12array = np.arange(1,8)array array([1, 2, 3, 4, 5, 6, 7]) 12zero = np.zeros((3,5))print(zero) [[0. 0. 0. 0. 0.] [0. 0. 0. 0. 0.] [0. 0. 0. 0. 0.]] 12one = np.ones((5,2))print(one) [[1. 1.] [1. 1.] [1. 1.] [1. 1.] [1. 1.]] #Pandas (영상, 이미지 등 비정형 데이터에서 주로 쓴다) 12import pandas as pdprint(pd.__version__) 1.1.5 ##구글 드라이브 연동 마운트하기 12from google.colab import drivedrive.mount('/content/drive') Mounted at /content/drive #데이터 처리 데이터 불러오기 &amp; 조회 123lemonade = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/kaggle2portpolio/data/Lemonade2016.csv')lemonade.info()lemonade.head(5) &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 32 entries, 0 to 31 Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Date 31 non-null object 1 Location 32 non-null object 2 Lemon 32 non-null int64 3 Orange 32 non-null int64 4 Temperature 32 non-null int64 5 Leaflets 31 non-null float64 6 Price 32 non-null float64 dtypes: float64(2), int64(3), object(2) memory usage: 1.9+ KB .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Date Location Lemon Orange Temperature Leaflets Price 0 7/1/2016 Park 97 67 70 90.0 0.25 1 7/2/2016 Park 98 67 72 90.0 0.25 2 7/3/2016 Park 110 77 71 104.0 0.25 3 7/4/2016 Beach 134 99 76 98.0 0.25 4 7/5/2016 Beach 159 118 78 135.0 0.25 요약 1lemonade.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Lemon Orange Temperature Leaflets Price count 32.000000 32.000000 32.000000 31.000000 32.000000 mean 116.156250 80.000000 78.968750 108.548387 0.354687 std 25.823357 21.863211 4.067847 20.117718 0.113137 min 71.000000 42.000000 70.000000 68.000000 0.250000 25% 98.000000 66.750000 77.000000 90.000000 0.250000 50% 113.500000 76.500000 80.500000 108.000000 0.350000 75% 131.750000 95.000000 82.000000 124.000000 0.500000 max 176.000000 129.000000 84.000000 158.000000 0.500000 값 가져오기 123lemonade['Location'] # 내용보기lemonade['Location'].value_counts() # 카운트 Beach 17 Park 15 Name: Location, dtype: int64 값 수정 12lemonade['Price'] = lemonade['Lemon'] + lemonade['Orange']lemonade.head(5) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Date Location Lemon Orange Temperature Leaflets Price 0 7/1/2016 Park 97 67 70 90.0 164 1 7/2/2016 Park 98 67 72 90.0 165 2 7/3/2016 Park 110 77 71 104.0 187 3 7/4/2016 Beach 134 99 76 98.0 233 4 7/5/2016 Beach 159 118 78 135.0 277 칼럼 추가 12lemonade['Test'] = lemonade['Price'] + lemonade['Leaflets']lemonade.head(5) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Date Location Lemon Orange Temperature Leaflets Price Test 0 7/1/2016 Park 97 67 70 90.0 164 254.0 1 7/2/2016 Park 98 67 72 90.0 165 255.0 2 7/3/2016 Park 110 77 71 104.0 187 291.0 3 7/4/2016 Beach 134 99 76 98.0 233 331.0 4 7/5/2016 Beach 159 118 78 135.0 277 412.0 칼럼 안보이게 출력 123lemonade_column_drop = lemonade.drop('Test',axis=1)#print(lemonade_column_drop.head(3))lemonade.head(3) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Date Location Lemon Orange Temperature Leaflets Price Test 0 7/1/2016 Park 97 67 70 90.0 164 254.0 1 7/2/2016 Park 98 67 72 90.0 165 255.0 2 7/3/2016 Park 110 77 71 104.0 187 291.0 칼럼크기 조절 12pd.set_option('display.max_columns', 8)print(lemonade.head(5)) Date Location Lemon Orange Temperature Leaflets Price Test 0 7/1/2016 Park 97 67 70 90.0 164 254.0 1 7/2/2016 Park 98 67 72 90.0 165 255.0 2 7/3/2016 Park 110 77 71 104.0 187 291.0 3 7/4/2016 Beach 134 99 76 98.0 233 331.0 4 7/5/2016 Beach 159 118 78 135.0 277 412.0 행 조절 1print(lemonade[0:5]) Date Location Lemon Orange Temperature Leaflets Price Test 0 7/1/2016 Park 97 67 70 90.0 164 254.0 1 7/2/2016 Park 98 67 72 90.0 165 255.0 2 7/3/2016 Park 110 77 71 104.0 187 291.0 3 7/4/2016 Beach 134 99 76 98.0 233 331.0 4 7/5/2016 Beach 159 118 78 135.0 277 412.0 조건 1print(lemonade[lemonade['Location'] == 'Beach'].head(3)) Date Location Lemon Orange Temperature Leaflets Price Test 3 7/4/2016 Beach 134 99 76 98.0 233 331.0 4 7/5/2016 Beach 159 118 78 135.0 277 412.0 5 7/6/2016 Beach 103 69 82 90.0 172 262.0 #파이썬 시각화 블로그 연습 1234567891011121314import matplotlib.pyplot as pltdates = [ '2021-01-01', '2021-01-02', '2021-01-03', '2021-01-04', '2021-01-05', '2021-01-06', '2021-01-07', '2021-01-08', '2021-01-09', '2021-01-10']min_temperature = [20.7, 17.9, 18.8, 14.6, 15.8, 15.8, 15.8, 17.4, 21.8, 20.0]max_temperature = [34.7, 28.9, 31.8, 25.6, 28.8, 21.8, 22.8, 28.4, 30.8, 32.0]#fig = 도화지 axes = 그래프 pl = pyplotfig,axes = plt.subplots(nrows=1, ncols=1, figsize=(10,6))axes.plot(dates, min_temperature, label = 'Min Temperature')axes.plot(dates, max_temperature, label = 'Max Temperature')axes.legend()plt.show() 12345678910111213141516import matplotlib.pyplot as pltimport numpy as npimport calendarmonth_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]sold_list = [300, 400, 550, 900, 600, 960, 900, 910, 800, 700, 550, 450]fig, ax = plt.subplots(figsize=(10,6))plot = ax.bar(month_list, sold_list)#plt.xticks(month_list, calendar.month_name[1:13], rotation=90)ax.set_xticks(month_list)#x축 ax.set_xticklabels(calendar.month_name[1:13], rotation=90) #x 축 라벨 지정for rect in plot: print(rect) height = rect.get_height() ax.text(rect.get_x() + rect.get_width()/2., 1.002*height,'%d' % int(height), ha='center', va='bottom')plt.show() Rectangle(xy=(0.6, 0), width=0.8, height=300, angle=0) Rectangle(xy=(1.6, 0), width=0.8, height=400, angle=0) Rectangle(xy=(2.6, 0), width=0.8, height=550, angle=0) Rectangle(xy=(3.6, 0), width=0.8, height=900, angle=0) Rectangle(xy=(4.6, 0), width=0.8, height=600, angle=0) Rectangle(xy=(5.6, 0), width=0.8, height=960, angle=0) Rectangle(xy=(6.6, 0), width=0.8, height=900, angle=0) Rectangle(xy=(7.6, 0), width=0.8, height=910, angle=0) Rectangle(xy=(8.6, 0), width=0.8, height=800, angle=0) Rectangle(xy=(9.6, 0), width=0.8, height=700, angle=0) Rectangle(xy=(10.6, 0), width=0.8, height=550, angle=0) Rectangle(xy=(11.6, 0), width=0.8, height=450, angle=0) #페가블로그 코드 1234import matplotlib.pyplot as pltfrom matplotlib.ticker import (MultipleLocator, AutoMinorLocator, FuncFormatter)import seaborn as snsimport numpy as np 123456789101112131415161718192021222324252627def plot_example(ax, zorder=0): ax.bar(tips_day[&quot;day&quot;], tips_day[&quot;tip&quot;], color=&quot;lightgray&quot;, zorder=zorder) ax.set_title(&quot;tip (mean)&quot;, fontsize=16, pad=12) # Values h_pad = 0.1 for i in range(4): fontweight = &quot;normal&quot; color = &quot;k&quot; if i == 3: fontweight = &quot;bold&quot; color = &quot;darkred&quot; ax.text(i, tips_day[&quot;tip&quot;].loc[i] + h_pad, f&quot;{tips_day['tip'].loc[i]:0.2f}&quot;, horizontalalignment='center', fontsize=12, fontweight=fontweight, color=color) # Sunday ax.patches[3].set_facecolor(&quot;darkred&quot;) ax.patches[3].set_edgecolor(&quot;black&quot;) # set_range ax.set_ylim(0, 4) return axdef major_formatter(x, pos): return &quot;{%.2f}&quot; % xformatter = FuncFormatter(major_formatter) 123tips = sns.load_dataset(&quot;tips&quot;)tips_day = tips.groupby(&quot;day&quot;).mean().reset_index()print(tips_day) day total_bill tip size 0 Thur 17.682742 2.771452 2.451613 1 Fri 17.151579 2.734737 2.105263 2 Sat 20.441379 2.993103 2.517241 3 Sun 21.410000 3.255132 2.842105 12fig, ax = plt.subplots(figsize=(10, 6))ax = plot_example(ax, zorder=2) 123456fig, ax = plt.subplots(figsize=(10, 6))ax = plot_example(ax, zorder=2)ax.spines[&quot;top&quot;].set_visible(False)ax.spines[&quot;right&quot;].set_visible(False)ax.spines[&quot;left&quot;].set_visible(False) 12345678910fig, ax = plt.subplots()ax = plot_example(ax, zorder=2)ax.spines[&quot;top&quot;].set_visible(False)ax.spines[&quot;right&quot;].set_visible(False)ax.spines[&quot;left&quot;].set_visible(False)ax.yaxis.set_major_locator(MultipleLocator(1))ax.yaxis.set_major_formatter(formatter)ax.yaxis.set_minor_locator(MultipleLocator(0.5)) 12345678910111213fig, ax = plt.subplots()ax = plot_example(ax, zorder=2)ax.spines[&quot;top&quot;].set_visible(False)ax.spines[&quot;right&quot;].set_visible(False)ax.spines[&quot;left&quot;].set_visible(False)ax.yaxis.set_major_locator(MultipleLocator(1))ax.yaxis.set_major_formatter(formatter)ax.yaxis.set_minor_locator(MultipleLocator(0.5)) ax.grid(axis=&quot;y&quot;, which=&quot;major&quot;, color=&quot;lightgray&quot;)ax.grid(axis=&quot;y&quot;, which=&quot;minor&quot;, ls=&quot;:&quot;) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556import matplotlib.pyplot as pltimport seaborn as snsimport numpy as nptips = sns.load_dataset(&quot;tips&quot;)fig, ax = plt.subplots(nrows = 1, ncols = 2, figsize=(16, 5))# Ideal Bar Graphax0 = sns.barplot(x = &quot;day&quot;, y = 'total_bill', data = tips, ci=None, color='lightgray', alpha=0.85, zorder=2, ax=ax[0])group_mean = tips.groupby(['day'])['total_bill'].agg('mean')h_day = group_mean.sort_values(ascending=False).index[0]h_mean = np.round(group_mean.sort_values(ascending=False)[0], 2)for p in ax0.patches: fontweight = &quot;normal&quot; color = &quot;k&quot; height = np.round(p.get_height(), 2) if h_mean == height: fontweight=&quot;bold&quot; color=&quot;darkred&quot; p.set_facecolor(color) p.set_edgecolor(&quot;black&quot;) ax0.text(p.get_x() + p.get_width()/2., height+1, height, ha = 'center', size=12, fontweight=fontweight, color=color)ax0.set_ylim(-3, 30)ax0.set_title(&quot;Ideal Bar Graph&quot;, size = 16)ax0.spines['top'].set_visible(False)ax0.spines['left'].set_position((&quot;outward&quot;, 20))ax0.spines['left'].set_visible(False)ax0.spines['right'].set_visible(False)ax0.set_xlabel(&quot;Weekday&quot;, fontsize=14)for xtick in ax0.get_xticklabels(): if xtick.get_text() == h_day: xtick.set_color(&quot;darkred&quot;) xtick.set_fontweight(&quot;demibold&quot;)ax0.set_xticklabels(['Thursday', 'Friday', 'Saturday', 'Sunday'], size=12)ax0.set_ylabel(&quot;Avg. Total Bill($)&quot;, fontsize=14)ax0.grid(axis=&quot;y&quot;, which=&quot;major&quot;, color=&quot;lightgray&quot;)ax0.grid(axis=&quot;y&quot;, which=&quot;minor&quot;, ls=&quot;:&quot;)ax1 = sns.barplot(x = &quot;day&quot;, y = 'total_bill', data = tips, ci=None, alpha=0.85, ax=ax[1])for p in ax1.patches: height = np.round(p.get_height(), 2) ax1.text(p.get_x() + p.get_width()/2., height+1, height, ha = 'center', size=12)ax1.set_ylim(-3, 30)ax1.set_title(&quot;Just Bar Graph&quot;)fig.show() 1","link":"/2021/04/07/Untitled0/"},{"title":"","text":"로지스틱 회귀 h1 {font-size: 34px;} h1.title {font-size: 38px;} h2 {font-size: 30px;} h3 {font-size: 24px;} h4 {font-size: 18px;} h5 {font-size: 16px;} h6 {font-size: 12px;} code {color: inherit; background-color: rgba(0, 0, 0, 0.04);} pre:not([class]) { background-color: white } code{white-space: pre-wrap;} span.smallcaps{font-variant: small-caps;} span.underline{text-decoration: underline;} div.column{display: inline-block; vertical-align: top; width: 50%;} div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;} ul.task-list{list-style: none;} code{white-space: pre;} if (window.hljs) { hljs.configure({languages: []}); hljs.initHighlightingOnLoad(); if (document.readyState && document.readyState === \"complete\") { window.setTimeout(function() { hljs.initHighlighting(); }, 0); } } .main-container { max-width: 940px; margin-left: auto; margin-right: auto; } img { max-width:100%; } .tabbed-pane { padding-top: 12px; } .html-widget { margin-bottom: 20px; } button.code-folding-btn:focus { outline: none; } summary { display: list-item; } pre code { padding: 0; } .tabset-dropdown > .nav-tabs { display: inline-table; max-height: 500px; min-height: 44px; overflow-y: auto; border: 1px solid #ddd; border-radius: 4px; } .tabset-dropdown > .nav-tabs > li.active:before { content: \"\"; font-family: 'Glyphicons Halflings'; display: inline-block; padding: 10px; border-right: 1px solid #ddd; } .tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before { content: \"\"; border: none; } .tabset-dropdown > .nav-tabs.nav-tabs-open:before { content: \"\"; font-family: 'Glyphicons Halflings'; display: inline-block; padding: 10px; border-right: 1px solid #ddd; } .tabset-dropdown > .nav-tabs > li.active { display: block; } .tabset-dropdown > .nav-tabs > li > a, .tabset-dropdown > .nav-tabs > li > a:focus, .tabset-dropdown > .nav-tabs > li > a:hover { border: none; display: inline-block; border-radius: 4px; background-color: transparent; } .tabset-dropdown > .nav-tabs.nav-tabs-open > li { display: block; float: none; } .tabset-dropdown > .nav-tabs > li { display: none; } 로지스틱 회귀 2021 3 31 library(ggplot2) library(cowplot) ## NOTE: The data used in this demo comes from the UCI machine learning ## repository. ## http://archive.ics.uci.edu/ml/index.php ## Specifically, this is the heart disease data set. ## http://archive.ics.uci.edu/ml/datasets/Heart+Disease url &lt;- &quot;http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data&quot; data &lt;- read.csv(url, header=FALSE) head(data) ## V1 V2 V3 V4 V5 V6 V7 V8 V9 V10 V11 V12 V13 V14 ## 1 63 1 1 145 233 1 2 150 0 2.3 3 0.0 6.0 0 ## 2 67 1 4 160 286 0 2 108 1 1.5 2 3.0 3.0 2 ## 3 67 1 4 120 229 0 2 129 1 2.6 2 2.0 7.0 1 ## 4 37 1 3 130 250 0 0 187 0 3.5 3 0.0 3.0 0 ## 5 41 0 2 130 204 0 2 172 0 1.4 1 0.0 3.0 0 ## 6 56 1 2 120 236 0 0 178 0 0.8 1 0.0 3.0 0 colnames(data) &lt;- c( &quot;age&quot;, &quot;sex&quot;,# 0 = female, 1 = male &quot;cp&quot;, # chest pain # 1 = typical angina, # 2 = atypical angina, # 3 = non-anginal pain, # 4 = asymptomatic &quot;trestbps&quot;, # resting blood pressure (in mm Hg) &quot;chol&quot;, # serum cholestoral in mg/dl &quot;fbs&quot;, # fasting blood sugar if less than 120 mg/dl, 1 = TRUE, 0 = FALSE &quot;restecg&quot;, # resting electrocardiographic results # 1 = normal # 2 = having ST-T wave abnormality # 3 = showing probable or definite left ventricular hypertrophy &quot;thalach&quot;, # maximum heart rate achieved &quot;exang&quot;, # exercise induced angina, 1 = yes, 0 = no &quot;oldpeak&quot;, # ST depression induced by exercise relative to rest &quot;slope&quot;, # the slope of the peak exercise ST segment # 1 = upsloping # 2 = flat # 3 = downsloping &quot;ca&quot;, # number of major vessels (0-3) colored by fluoroscopy &quot;thal&quot;, # this is short of thalium heart scan # 3 = normal (no cold spots) # 6 = fixed defect (cold spots during rest and exercise) # 7 = reversible defect (when cold spots only appear during exercise) &quot;hd&quot; # (the predicted attribute) - diagnosis of heart disease # 0 if less than or equal to 50% diameter narrowing # 1 if greater than 50% diameter narrowing ) head(data) # now we have data and column names ## age sex cp trestbps chol fbs restecg thalach exang oldpeak slope ca thal hd ## 1 63 1 1 145 233 1 2 150 0 2.3 3 0.0 6.0 0 ## 2 67 1 4 160 286 0 2 108 1 1.5 2 3.0 3.0 2 ## 3 67 1 4 120 229 0 2 129 1 2.6 2 2.0 7.0 1 ## 4 37 1 3 130 250 0 0 187 0 3.5 3 0.0 3.0 0 ## 5 41 0 2 130 204 0 2 172 0 1.4 1 0.0 3.0 0 ## 6 56 1 2 120 236 0 0 178 0 0.8 1 0.0 3.0 0 str(data) # this shows that we need to tell R which columns contain factors ## 'data.frame': 303 obs. of 14 variables: ## $ age : num 63 67 67 37 41 56 62 57 63 53 ... ## $ sex : num 1 1 1 1 0 1 0 0 1 1 ... ## $ cp : num 1 4 4 3 2 2 4 4 4 4 ... ## $ trestbps: num 145 160 120 130 130 120 140 120 130 140 ... ## $ chol : num 233 286 229 250 204 236 268 354 254 203 ... ## $ fbs : num 1 0 0 0 0 0 0 0 0 1 ... ## $ restecg : num 2 2 2 0 2 0 2 0 2 2 ... ## $ thalach : num 150 108 129 187 172 178 160 163 147 155 ... ## $ exang : num 0 1 1 0 0 0 0 1 0 1 ... ## $ oldpeak : num 2.3 1.5 2.6 3.5 1.4 0.8 3.6 0.6 1.4 3.1 ... ## $ slope : num 3 2 2 3 1 1 3 1 2 3 ... ## $ ca : chr &quot;0.0&quot; &quot;3.0&quot; &quot;2.0&quot; &quot;0.0&quot; ... ## $ thal : chr &quot;6.0&quot; &quot;3.0&quot; &quot;7.0&quot; &quot;3.0&quot; ... ## $ hd : int 0 2 1 0 0 0 3 0 2 1 ... # it also shows us that there are some missing values. There are &quot;?&quot;s # in the dataset. These are in the &quot;ca&quot; and &quot;thal&quot; columns... ## First, convert &quot;?&quot;s to NAs... data[data == &quot;?&quot;] &lt;- NA ## Now add factors for variables that are factors and clean up the factors ## that had missing data... data[data$sex == 0,]$sex &lt;- &quot;F&quot; data[data$sex == 1,]$sex &lt;- &quot;M&quot; data$sex &lt;- as.factor(data$sex) data$cp &lt;- as.factor(data$cp) data$fbs &lt;- as.factor(data$fbs) data$restecg &lt;- as.factor(data$restecg) data$exang &lt;- as.factor(data$exang) data$slope &lt;- as.factor(data$slope) data$ca &lt;- as.integer(data$ca) # since this column had &quot;?&quot;s in it # R thinks that the levels for the factor are strings, but # we know they are integers, so first convert the strings to integers... data$ca &lt;- as.factor(data$ca) # ...then convert the integers to factor levels data$thal &lt;- as.integer(data$thal) # &quot;thal&quot; also had &quot;?&quot;s in it. data$thal &lt;- as.factor(data$thal) ## This next line replaces 0 and 1 with &quot;Healthy&quot; and &quot;Unhealthy&quot; data$hd &lt;- ifelse(test=data$hd == 0, yes=&quot;Healthy&quot;, no=&quot;Unhealthy&quot;) data$hd &lt;- as.factor(data$hd) # Now convert to a factor str(data) ## this shows that the correct columns are factors ## 'data.frame': 303 obs. of 14 variables: ## $ age : num 63 67 67 37 41 56 62 57 63 53 ... ## $ sex : Factor w/ 2 levels &quot;F&quot;,&quot;M&quot;: 2 2 2 2 1 2 1 1 2 2 ... ## $ cp : Factor w/ 4 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;: 1 4 4 3 2 2 4 4 4 4 ... ## $ trestbps: num 145 160 120 130 130 120 140 120 130 140 ... ## $ chol : num 233 286 229 250 204 236 268 354 254 203 ... ## $ fbs : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 2 1 1 1 1 1 1 1 1 2 ... ## $ restecg : Factor w/ 3 levels &quot;0&quot;,&quot;1&quot;,&quot;2&quot;: 3 3 3 1 3 1 3 1 3 3 ... ## $ thalach : num 150 108 129 187 172 178 160 163 147 155 ... ## $ exang : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 1 2 2 1 1 1 1 2 1 2 ... ## $ oldpeak : num 2.3 1.5 2.6 3.5 1.4 0.8 3.6 0.6 1.4 3.1 ... ## $ slope : Factor w/ 3 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;: 3 2 2 3 1 1 3 1 2 3 ... ## $ ca : Factor w/ 4 levels &quot;0&quot;,&quot;1&quot;,&quot;2&quot;,&quot;3&quot;: 1 4 3 1 1 1 3 1 2 1 ... ## $ thal : Factor w/ 3 levels &quot;3&quot;,&quot;6&quot;,&quot;7&quot;: 2 1 3 1 1 1 1 1 3 3 ... ## $ hd : Factor w/ 2 levels &quot;Healthy&quot;,&quot;Unhealthy&quot;: 1 2 2 1 1 1 2 1 2 2 ... ## Now determine how many rows have &quot;NA&quot; (aka &quot;Missing data&quot;). If it's just ## a few, we can remove them from the dataset, otherwise we should consider ## imputing the values with a Random Forest or some other imputation method. nrow(data[is.na(data$ca) | is.na(data$thal),]) ## [1] 6 data[is.na(data$ca) | is.na(data$thal),] ## age sex cp trestbps chol fbs restecg thalach exang oldpeak slope ca thal ## 88 53 F 3 128 216 0 2 115 0 0.0 1 0 &lt;NA&gt; ## 167 52 M 3 138 223 0 0 169 0 0.0 1 &lt;NA&gt; 3 ## 193 43 M 4 132 247 1 2 143 1 0.1 2 &lt;NA&gt; 7 ## 267 52 M 4 128 204 1 0 156 1 1.0 2 0 &lt;NA&gt; ## 288 58 M 2 125 220 0 0 144 0 0.4 2 &lt;NA&gt; 7 ## 303 38 M 3 138 175 0 0 173 0 0.0 1 &lt;NA&gt; 3 ## hd ## 88 Healthy ## 167 Healthy ## 193 Unhealthy ## 267 Unhealthy ## 288 Healthy ## 303 Healthy ## so 6 of the 303 rows of data have missing values. This isn't a large ## percentage (2%), so we can just remove them from the dataset ## NOTE: This is different from when we did machine learning with ## Random Forests. When we did that, we imputed values. nrow(data) ## [1] 303 data &lt;- data[!(is.na(data$ca) | is.na(data$thal)),] nrow(data) ## [1] 297 ##################################### ## ## Now we can do some quality control by making sure all of the factor ## levels are represented by people with and without heart disease (hd) ## ## NOTE: We also want to exclude variables that only have 1 or 2 samples in ## a category since +/- one or two samples can have a large effect on the ## odds/log(odds) ## ## ##################################### xtabs(~ hd + sex, data=data) ## sex ## hd F M ## Healthy 71 89 ## Unhealthy 25 112 xtabs(~ hd + cp, data=data) ## cp ## hd 1 2 3 4 ## Healthy 16 40 65 39 ## Unhealthy 7 9 18 103 xtabs(~ hd + fbs, data=data) ## fbs ## hd 0 1 ## Healthy 137 23 ## Unhealthy 117 20 xtabs(~ hd + restecg, data=data) ## restecg ## hd 0 1 2 ## Healthy 92 1 67 ## Unhealthy 55 3 79 xtabs(~ hd + exang, data=data) ## exang ## hd 0 1 ## Healthy 137 23 ## Unhealthy 63 74 xtabs(~ hd + slope, data=data) ## slope ## hd 1 2 3 ## Healthy 103 48 9 ## Unhealthy 36 89 12 xtabs(~ hd + ca, data=data) ## ca ## hd 0 1 2 3 ## Healthy 129 21 7 3 ## Unhealthy 45 44 31 17 xtabs(~ hd + thal, data=data) ## thal ## hd 3 6 7 ## Healthy 127 6 27 ## Unhealthy 37 12 88 ##################################### ## ## Now we are ready for some logistic regression. First we'll create a very ## simple model that uses sex to predict heart disease ## ##################################### ## let's start super simple and see if sex (female/male) is a good ## predictor... ## First, let's just look at the raw data... xtabs(~ hd + sex, data=data) ## sex ## hd F M ## Healthy 71 89 ## Unhealthy 25 112 # sex # hd F M # Healthy 71 89 # Unhealthy 25 112 ## Most of the females are healthy and most of the males are unhealthy. ## Being female is likely to decrease the odds in being unhealthy. ## In other words, if a sample is female, the odds are against it that it ## will be unhealthy ## Being male is likely to increase the odds in being unhealthy... ## In other words, if a sample is male, the odds are for it being unhealthy ########### ## ## Now do the actual logistic regression ## ########### logistic &lt;- glm(hd ~ sex, data=data, family=&quot;binomial&quot;) #glm함수에 일반화된 선형모델을 먼저 수행하는 함수, hd ~ sex :성별을 사용하여 심장질환 예측 지정정 ,data=data : 모델에사용될 데이터지정, family=&quot;binomial&quot; : 일반화 된 선형의 이항패밀리 #logistic &lt;- glm(hd ~ sex, data=data, family=&quot;binomial&quot;) 여기 물결표는 심장 질환 HD를 모델링 한다는 의미 summary(logistic) ## ## Call: ## glm(formula = hd ~ sex, family = &quot;binomial&quot;, data = data) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.2765 -1.2765 -0.7768 1.0815 1.6404 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.0438 0.2326 -4.488 7.18e-06 *** ## sexM 1.2737 0.2725 4.674 2.95e-06 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 409.95 on 296 degrees of freedom ## Residual deviance: 386.12 on 295 degrees of freedom ## AIC: 390.12 ## ## Number of Fisher Scoring iterations: 4 ## (Intercept) -1.0438 0.2326 -4.488 7.18e-06 *** ## sexM 1.2737 0.2725 4.674 2.95e-06 *** ## Let's start by going through the first coefficient... ## (Intercept) -1.0438 0.2326 -4.488 7.18e-06 *** ## ## The intercept is the log(odds) a female will be unhealthy. This is because ## female is the first factor in &quot;sex&quot; (the factors are ordered, ## alphabetically by default,&quot;female&quot;, &quot;male&quot;) female.log.odds &lt;- log(25 / 71) female.log.odds ## [1] -1.043804 ## Now let's look at the second coefficient... ## sexM 1.2737 0.2725 4.674 2.95e-06 *** ## ## sexM is the log(odds ratio) that tells us that if a sample has sex=M, the ## odds of being unhealthy are, on a log scale, 1.27 times greater than if ## a sample has sex=F. male.log.odds.ratio &lt;- log((112 / 89) / (25/71)) male.log.odds.ratio ## [1] 1.273667 ## Now calculate the overall &quot;Pseudo R-squared&quot; and its p-value ## NOTE: Since we are doing logistic regression... ## Null devaince = 2*(0 - LogLikelihood(null model)) ## = -2*LogLikihood(null model) ## Residual deviacne = 2*(0 - LogLikelihood(proposed model)) ## = -2*LogLikelihood(proposed model) ll.null &lt;- logistic$null.deviance/-2 ll.proposed &lt;- logistic$deviance/-2 ## McFadden's Pseudo R^2 = [ LL(Null) - LL(Proposed) ] / LL(Null) (ll.null - ll.proposed) / ll.null ## [1] 0.05812569 ## chi-square value = 2*(LL(Proposed) - LL(Null)) ## p-value = 1 - pchisq(chi-square value, df = 2-1) 1 - pchisq(2*(ll.proposed - ll.null), df=1) ## [1] 1.053157e-06 1 - pchisq((logistic$null.deviance - logistic$deviance), df=1) ## [1] 1.053157e-06 ## Lastly, let's see what this logistic regression predicts, given ## that a patient is either female or male (and no other data about them). predicted.data &lt;- data.frame( probability.of.hd=logistic$fitted.values, sex=data$sex) ## We can plot the data... ggplot(data=predicted.data, aes(x=sex, y=probability.of.hd)) + geom_point(aes(color=sex), size=5) + xlab(&quot;Sex&quot;) + ylab(&quot;Predicted probability of getting heart disease&quot;) ## Since there are only two probabilities (one for females and one for males), ## we can use a table to summarize the predicted probabilities. xtabs(~ probability.of.hd + sex, data=predicted.data) ## sex ## probability.of.hd F M ## 0.260416666667241 96 0 ## 0.55721393034826 0 201 ##################################### ## ## Now we will use all of the data available to predict heart disease ## ##################################### logistic &lt;- glm(hd ~ ., data=data, family=&quot;binomial&quot;) summary(logistic) ## ## Call: ## glm(formula = hd ~ ., family = &quot;binomial&quot;, data = data) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -3.0490 -0.4847 -0.1213 0.3039 2.9086 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -6.253978 2.960399 -2.113 0.034640 * ## age -0.023508 0.025122 -0.936 0.349402 ## sexM 1.670152 0.552486 3.023 0.002503 ** ## cp2 1.448396 0.809136 1.790 0.073446 . ## cp3 0.393353 0.700338 0.562 0.574347 ## cp4 2.373287 0.709094 3.347 0.000817 *** ## trestbps 0.027720 0.011748 2.359 0.018300 * ## chol 0.004445 0.004091 1.087 0.277253 ## fbs1 -0.574079 0.592539 -0.969 0.332622 ## restecg1 1.000887 2.638393 0.379 0.704424 ## restecg2 0.486408 0.396327 1.227 0.219713 ## thalach -0.019695 0.011717 -1.681 0.092781 . ## exang1 0.653306 0.447445 1.460 0.144267 ## oldpeak 0.390679 0.239173 1.633 0.102373 ## slope2 1.302289 0.486197 2.679 0.007395 ** ## slope3 0.606760 0.939324 0.646 0.518309 ## ca1 2.237444 0.514770 4.346 1.38e-05 *** ## ca2 3.271852 0.785123 4.167 3.08e-05 *** ## ca3 2.188715 0.928644 2.357 0.018428 * ## thal6 -0.168439 0.810310 -0.208 0.835331 ## thal7 1.433319 0.440567 3.253 0.001141 ** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 409.95 on 296 degrees of freedom ## Residual deviance: 183.10 on 276 degrees of freedom ## AIC: 225.1 ## ## Number of Fisher Scoring iterations: 6 ## Now calculate the overall &quot;Pseudo R-squared&quot; and its p-value ll.null &lt;- logistic$null.deviance/-2 ll.proposed &lt;- logistic$deviance/-2 ## McFadden's Pseudo R^2 = [ LL(Null) - LL(Proposed) ] / LL(Null) (ll.null - ll.proposed) / ll.null ## [1] 0.5533531 ## The p-value for the R^2 1 - pchisq(2*(ll.proposed - ll.null), df=(length(logistic$coefficients)-1)) ## [1] 0 ## now we can plot the data predicted.data &lt;- data.frame( probability.of.hd=logistic$fitted.values, hd=data$hd) predicted.data &lt;- predicted.data[ order(predicted.data$probability.of.hd, decreasing=FALSE),] predicted.data$rank &lt;- 1:nrow(predicted.data) ## Lastly, we can plot the predicted probabilities for each sample having ## heart disease and color by whether or not they actually had heart disease ggplot(data=predicted.data, aes(x=rank, y=probability.of.hd)) + geom_point(aes(color=hd), alpha=1, shape=4, stroke=2) + xlab(&quot;Index&quot;) + ylab(&quot;Predicted probability of getting heart disease&quot;) library(mlogit) ## Loading required package: dfidx ## ## Attaching package: 'dfidx' ## The following object is masked from 'package:stats': ## ## filter data(&quot;Fishing&quot;, package = &quot;mlogit&quot;) Fish &lt;- dfidx(Fishing, varying = 2:9, shape = &quot;wide&quot;, choice = &quot;mode&quot;) ## a pure &quot;conditional&quot; model summary(mlogit(mode ~ price + catch, data = Fish)) ## ## Call: ## mlogit(formula = mode ~ price + catch, data = Fish, method = &quot;nr&quot;) ## ## Frequencies of alternatives:choice ## beach boat charter pier ## 0.11337 0.35364 0.38240 0.15059 ## ## nr method ## 7 iterations, 0h:0m:0s ## g'(-H)^-1g = 6.22E-06 ## successive function values within tolerance limits ## ## Coefficients : ## Estimate Std. Error z-value Pr(&gt;|z|) ## (Intercept):boat 0.8713749 0.1140428 7.6408 2.154e-14 *** ## (Intercept):charter 1.4988884 0.1329328 11.2755 &lt; 2.2e-16 *** ## (Intercept):pier 0.3070552 0.1145738 2.6800 0.0073627 ** ## price -0.0247896 0.0017044 -14.5444 &lt; 2.2e-16 *** ## catch 0.3771689 0.1099707 3.4297 0.0006042 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Log-Likelihood: -1230.8 ## McFadden R^2: 0.17823 ## Likelihood ratio test : chisq = 533.88 (p.value = &lt; 2.22e-16) // add bootstrap table styles to pandoc tables function bootstrapStylePandocTables() { $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed'); } $(document).ready(function () { bootstrapStylePandocTables(); }); $(document).ready(function () { window.buildTabsets(\"TOC\"); }); $(document).ready(function () { $('.tabset-dropdown > .nav-tabs > li').click(function () { $(this).parent().toggleClass('nav-tabs-open'); }); }); (function () { var script = document.createElement(\"script\"); script.type = \"text/javascript\"; script.src = \"https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\"; document.getElementsByTagName(\"head\")[0].appendChild(script); })();","link":"/2021/04/06/Logistic-regression2/"},{"title":"","text":"20210323 내용 h1 {font-size: 34px;} h1.title {font-size: 38px;} h2 {font-size: 30px;} h3 {font-size: 24px;} h4 {font-size: 18px;} h5 {font-size: 16px;} h6 {font-size: 12px;} code {color: inherit; background-color: rgba(0, 0, 0, 0.04);} pre:not([class]) { background-color: white } code{white-space: pre-wrap;} span.smallcaps{font-variant: small-caps;} span.underline{text-decoration: underline;} div.column{display: inline-block; vertical-align: top; width: 50%;} div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;} ul.task-list{list-style: none;} code{white-space: pre;} if (window.hljs) { hljs.configure({languages: []}); hljs.initHighlightingOnLoad(); if (document.readyState && document.readyState === \"complete\") { window.setTimeout(function() { hljs.initHighlighting(); }, 0); } } .main-container { max-width: 940px; margin-left: auto; margin-right: auto; } img { max-width:100%; } .tabbed-pane { padding-top: 12px; } .html-widget { margin-bottom: 20px; } button.code-folding-btn:focus { outline: none; } summary { display: list-item; } pre code { padding: 0; } .tabset-dropdown > .nav-tabs { display: inline-table; max-height: 500px; min-height: 44px; overflow-y: auto; border: 1px solid #ddd; border-radius: 4px; } .tabset-dropdown > .nav-tabs > li.active:before { content: \"\"; font-family: 'Glyphicons Halflings'; display: inline-block; padding: 10px; border-right: 1px solid #ddd; } .tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before { content: \"\"; border: none; } .tabset-dropdown > .nav-tabs.nav-tabs-open:before { content: \"\"; font-family: 'Glyphicons Halflings'; display: inline-block; padding: 10px; border-right: 1px solid #ddd; } .tabset-dropdown > .nav-tabs > li.active { display: block; } .tabset-dropdown > .nav-tabs > li > a, .tabset-dropdown > .nav-tabs > li > a:focus, .tabset-dropdown > .nav-tabs > li > a:hover { border: none; display: inline-block; border-radius: 4px; background-color: transparent; } .tabset-dropdown > .nav-tabs.nav-tabs-open > li { display: block; float: none; } .tabset-dropdown > .nav-tabs > li { display: none; } 20210323 내용 조심재 2021 3 23 문제 1. 패키지 설치 방법 ggplot2와 dplyr 패키지 설치 방법을 기재하세요. library(ggplot2) library(dplyr) ## ## Attaching package: 'dplyr' ## The following objects are masked from 'package:stats': ## ## filter, lag ## The following objects are masked from 'package:base': ## ## intersect, setdiff, setequal, union 문제 2. 질적 변수와 양적 변수의 차이에 대해 설명 하세요 문제 3. 엑셀 데이터를 불러오세요. counties &lt;- readxl::read_xlsx(&quot;counties.xslx&quot;, sheet = 1) glimpse(counties) ## Rows: 3,138 ## Columns: 40 ## $ census_id &lt;chr&gt; &quot;1001&quot;, &quot;1003&quot;, &quot;1005&quot;, &quot;1007&quot;, &quot;1009&quot;, &quot;1011&quot;, &quot;10~ ## $ state &lt;chr&gt; &quot;Alabama&quot;, &quot;Alabama&quot;, &quot;Alabama&quot;, &quot;Alabama&quot;, &quot;Alabam~ ## $ county &lt;chr&gt; &quot;Autauga&quot;, &quot;Baldwin&quot;, &quot;Barbour&quot;, &quot;Bibb&quot;, &quot;Blount&quot;, ~ ## $ region &lt;chr&gt; &quot;South&quot;, &quot;South&quot;, &quot;South&quot;, &quot;South&quot;, &quot;South&quot;, &quot;South~ ## $ metro &lt;chr&gt; &quot;Metro&quot;, &quot;Metro&quot;, &quot;Nonmetro&quot;, &quot;Metro&quot;, &quot;Metro&quot;, &quot;No~ ## $ population &lt;dbl&gt; 55221, 195121, 26932, 22604, 57710, 10678, 20354, 1~ ## $ men &lt;dbl&gt; 26745, 95314, 14497, 12073, 28512, 5660, 9502, 5627~ ## $ women &lt;dbl&gt; 28476, 99807, 12435, 10531, 29198, 5018, 10852, 603~ ## $ hispanic &lt;dbl&gt; 2.6, 4.5, 4.6, 2.2, 8.6, 4.4, 1.2, 3.5, 0.4, 1.5, 7~ ## $ white &lt;dbl&gt; 75.8, 83.1, 46.2, 74.5, 87.9, 22.2, 53.3, 73.0, 57.~ ## $ black &lt;dbl&gt; 18.5, 9.5, 46.7, 21.4, 1.5, 70.7, 43.8, 20.3, 40.3,~ ## $ native &lt;dbl&gt; 0.4, 0.6, 0.2, 0.4, 0.3, 1.2, 0.1, 0.2, 0.2, 0.6, 0~ ## $ asian &lt;dbl&gt; 1.0, 0.7, 0.4, 0.1, 0.1, 0.2, 0.4, 0.9, 0.8, 0.3, 0~ ## $ pacific &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0~ ## $ citizens &lt;dbl&gt; 40725, 147695, 20714, 17495, 42345, 8057, 15581, 88~ ## $ income &lt;dbl&gt; 51281, 50254, 32964, 38678, 45813, 31938, 32229, 41~ ## $ income_err &lt;dbl&gt; 2391, 1263, 2973, 3995, 3141, 5884, 1793, 925, 2949~ ## $ income_per_cap &lt;dbl&gt; 24974, 27317, 16824, 18431, 20532, 17580, 18390, 21~ ## $ income_per_cap_err &lt;dbl&gt; 1080, 711, 798, 1618, 708, 2055, 714, 489, 1366, 15~ ## $ poverty &lt;dbl&gt; 12.9, 13.4, 26.7, 16.8, 16.7, 24.6, 25.4, 20.5, 21.~ ## $ child_poverty &lt;dbl&gt; 18.6, 19.2, 45.3, 27.9, 27.2, 38.4, 39.2, 31.6, 37.~ ## $ professional &lt;dbl&gt; 33.2, 33.1, 26.8, 21.5, 28.5, 18.8, 27.5, 27.3, 23.~ ## $ service &lt;dbl&gt; 17.0, 17.7, 16.1, 17.9, 14.1, 15.0, 16.6, 17.7, 14.~ ## $ office &lt;dbl&gt; 24.2, 27.1, 23.1, 17.8, 23.9, 19.7, 21.9, 24.2, 26.~ ## $ construction &lt;dbl&gt; 8.6, 10.8, 10.8, 19.0, 13.5, 20.1, 10.3, 10.5, 11.5~ ## $ production &lt;dbl&gt; 17.1, 11.2, 23.1, 23.7, 19.9, 26.4, 23.7, 20.4, 24.~ ## $ drive &lt;dbl&gt; 87.5, 84.7, 83.8, 83.2, 84.9, 74.9, 84.5, 85.3, 85.~ ## $ carpool &lt;dbl&gt; 8.8, 8.8, 10.9, 13.5, 11.2, 14.9, 12.4, 9.4, 11.9, ~ ## $ transit &lt;dbl&gt; 0.1, 0.1, 0.4, 0.5, 0.4, 0.7, 0.0, 0.2, 0.2, 0.2, 0~ ## $ walk &lt;dbl&gt; 0.5, 1.0, 1.8, 0.6, 0.9, 5.0, 0.8, 1.2, 0.3, 0.6, 1~ ## $ other_transp &lt;dbl&gt; 1.3, 1.4, 1.5, 1.5, 0.4, 1.7, 0.6, 1.2, 0.4, 0.7, 1~ ## $ work_at_home &lt;dbl&gt; 1.8, 3.9, 1.6, 0.7, 2.3, 2.8, 1.7, 2.7, 2.1, 2.5, 1~ ## $ mean_commute &lt;dbl&gt; 26.5, 26.4, 24.1, 28.8, 34.9, 27.5, 24.6, 24.1, 25.~ ## $ employed &lt;dbl&gt; 23986, 85953, 8597, 8294, 22189, 3865, 7813, 47401,~ ## $ private_work &lt;dbl&gt; 73.6, 81.5, 71.8, 76.8, 82.0, 79.5, 77.4, 74.1, 85.~ ## $ public_work &lt;dbl&gt; 20.9, 12.3, 20.8, 16.1, 13.5, 15.1, 16.2, 20.8, 12.~ ## $ self_employed &lt;dbl&gt; 5.5, 5.8, 7.3, 6.7, 4.2, 5.4, 6.2, 5.0, 2.8, 7.9, 4~ ## $ family_work &lt;dbl&gt; 0.0, 0.4, 0.1, 0.4, 0.4, 0.0, 0.2, 0.1, 0.0, 0.5, 0~ ## $ unemployment &lt;dbl&gt; 7.6, 7.5, 17.6, 8.3, 7.7, 18.0, 10.9, 12.3, 8.9, 7.~ ## $ land_area &lt;dbl&gt; 594.44, 1589.78, 884.88, 622.58, 644.78, 622.81, 77~ 문제 4. private_work, unemployment를 활용하여 산점도를 작성하세요. 조건: region을 기준으로 그룹화를 진행합니다. ggplot(counties, aes(x = private_work, y = unemployment, colour = region)) + geom_point() 문제 5. dplyr 함수를 활용하여, 아래 데이터를 요약하세요. counties 데이터를 활용합니다. 변수 추출은 region, state, men, women 각 region, state 별 men, women 전체 인구수를 구합니다. 최종 데이터셋 저장 이름은 final_df로 명명합니다. counties %&gt;% select(region, state, county, men, women) %&gt;% group_by(region, state) %&gt;% summarize(total_men = sum(men), total_women = sum(women)) %&gt;% filter(region == &quot;North Central&quot;) %&gt;% arrange(desc(total_men)) -&gt; final_df ## `summarise()` has grouped output by 'region'. You can override using the `.groups` argument. glimpse(final_df) ## Rows: 12 ## Columns: 4 ## Groups: region [1] ## $ region &lt;chr&gt; &quot;North Central&quot;, &quot;North Central&quot;, &quot;North Central&quot;, &quot;North ~ ## $ state &lt;chr&gt; &quot;Illinois&quot;, &quot;Ohio&quot;, &quot;Michigan&quot;, &quot;Indiana&quot;, &quot;Missouri&quot;, &quot;Wi~ ## $ total_men &lt;dbl&gt; 6316899, 5662893, 4861973, 3235263, 2964003, 2851385, 2692~ ## $ total_women &lt;dbl&gt; 6556862, 5913084, 5038598, 3333382, 3081445, 2890732, 2727~ 문제 6. final_df를 기준으로 막대 그래프를 그립니다. x축: state 1개의 region만 선택 / 선택은 자유 색상 꾸미기 등은 자유입니다. 그래프 정렬도 안해도 됩니다. ggplot(final_df, aes(x = state, y = total_men)) + geom_col() // add bootstrap table styles to pandoc tables function bootstrapStylePandocTables() { $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed'); } $(document).ready(function () { bootstrapStylePandocTables(); }); $(document).ready(function () { window.buildTabsets(\"TOC\"); }); $(document).ready(function () { $('.tabset-dropdown > .nav-tabs > li').click(function () { $(this).parent().toggleClass('nav-tabs-open'); }); }); (function () { var script = document.createElement(\"script\"); script.type = \"text/javascript\"; script.src = \"https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\"; document.getElementsByTagName(\"head\")[0].appendChild(script); })();","link":"/2021/03/23/20210323/"},{"title":"","text":"train / test 분리하는 이유? 머신러닝 모델을 학습하고 그 결과를 검증하기 위해서는 원래의 데이터를 Training, Validation, Testing의 용도로 나누어 다뤄야 한다. 그렇지 않고 Training에 사용한 데이터를 검증용으로 사용하면 시험문제를 알고 있는 상태에서 공부를 하고 그 지식을 바탕으로 시험을 치루는 꼴이므로 제대로 된 검증이 이루어지지 않기 때문이다. 사이킷런(scikit-learn)의 model_selection 패키지 안에 train_test_split 모듈을 활용하여 손쉽게 train set(학습 데이터 셋)과 test set(테스트 셋)을 분리할 수 있습니다. train / test 가 아닌 train / validation 으로 볼 수 있습니다. (어차피 용어의 차이긴 하지만요) 머신러닝 모델에 train 데이터를 100% 학습시킨 후 test 데이터에 모델을 적용했을 때 성능이 생각보다 안나오는 경우가 많습니다 (거의 99.999% 는요) 이러한 현상을 보통 Overfitting 되었다라고 합니다. 즉, 모델이 내가 가진 학습 데이터에 너무 과적합되도록 학습한 나머지, 이를 조금이라도 벗어난 케이스에 대해서는 예측율이 현저히 떨어지기 때문이라고 이해하시면 됩니다. 그렇기 때문에 Overfitting을 방지하는 것은 전체적인 모델 성능을 따져보았을 때 매우 중요한 프로세스 중 하나입니다. 기존 train / test로 구분 되어 있었던 데이터 셋을 train에서 train / validation으로 일정 비율 쪼갠 다음에 학습 시에는 train 셋으로 학습 후 중간중간 validation 셋으로 내가 학습한 모델 평가를 해주는 것입니다. 만약, 모델이 과적합되었다면, validation 셋으로 검증시 예측율이나 오차율이 떨어지는 현상을 확인할 수 있으며, 이런 현상이 나타나면 학습을 종료합니다. train_test_split 함수를 통해 단 1줄로 깔끔하게 분리할 수 있습니다. ※ 옵션 test_size: 테스트 셋 구성의 비율을 나타냅니다. train_size의 옵션과 반대 관계에 있는 옵션 값이며, 주로 test_size를 지정해 줍니다. 0.2는 전체 데이터 셋의 20%를 test (validation) 셋으로 지정하겠다는 의미입니다. default 값은 0.25 입니다. shuffle: default=True 입니다. split을 해주기 이전에 섞을건지 여부입니다. 보통은 default 값으로 놔둡니다. stratify: default=None 입니다. classification을 다룰 때 매우 중요한 옵션값입니다. stratify 값을 target으로 지정해주면 각각의 class 비율(ratio)을 train / validation에 유지해 줍니다. (한 쪽에 쏠려서 분배되는 것을 방지합니다) 만약 이 옵션을 지정해 주지 않고 classification 문제를 다룬다면, 성능의 차이가 많이 날 수 있습니다. random_state: 세트를 섞을 때 해당 int 값을 보고 섞으며, 하이퍼 파라미터를 튜닝시 이 값을 고정해두고 튜닝해야 매번 데이터셋이 변경되는 것을 방지할 수 있습니다.","link":"/2021/04/12/20210412/"},{"title":"","text":"fit / predict? #A.데이터 학습시키기clf = svm.SVC()clf.fit(data,label)#B. 데이터 예측하기pre = clf.predict(data)print(“예측결과:”,pre)A에서는 SVM이라는 알고리즘을 사용해 머신러닝을 수행한다. SVM객체를 만들고 fit() 메서드를 사용해 데이터를 학습시킨다. fit() 메서드는 첫번째 매개변수로 학습할 data 배열을 두번째 매개변수로는 label 배열을 전달한다.B에서는 학습한 결과를 기반으로 data를 예측한다.predict() 메서드에 예측하고 싶은 데이터 배열을 전달하면 데이터 수만큼 예측결과를 리턴해준다. 학습기계에 data를 학습시킨다. → fit() 메서드 data를 넣어 결과를 예측시킨다. → predict() 메서드 scikit-learn의 SVM(SVC) 알고리즘을 사용한다.fit()메서드를 학습하고 predict() 메서드로 예측한다.#A.학습 데이터 추출clf = svm.SVC()clf.fit(data[“images”],data[“labels”])#B. 테스트 데이터 예측predict= clf.predict(test[“images”])","link":"/2021/04/14/20210414/"},{"title":"","text":"Binary/ BaseN encoding 범주 정보를 인코딩하는 방법은 무한합니다. Category Encoder에있는 것은 대부분의 용도에 충분해야합니다 #OneHotEncoding - 매우 직관적이고 모든 범주형 자료에서 활용가능 - 차원이 피쳐의 수만큼 증가한다는 커다란 단점 - 가지고 있는 샘플 수가 많다면, 필요한 연산량과 메모리가 기하급수적으로 증가 #category_encoders Binary와 BaseN 인코딩 모두 sklearn의 Categorical Encoding Methods 중 하나※ 라이브러리 설치 pip install category_encoders import category_encoders as ce #BinaryEncoder 2진법으로 표현하는 것 피쳐의 갯수만큼 벡터의 길이가 늘어남 원핫인코딩이 가지는 메모리 부족3과 차원의 저주도 꽤나 잘 해결할 수 있다 numeric value로 바꿔주기 이진법 숫자로 바꿔주기 각 자릿수에 맞추어 컬럼을 만들어주기 #fit_transformfit과 transform의 기능을 합친 것입니다. 인코딩을 원하는 데이터를 집어넣는 첫 단계이며, 결과가 위에서 생성한 instance에 저장이 됩니다. 따라서 새로운 값을 마주할 때 차원을 유지하거나, 원래 값이 궁금할 때에 손쉽게 확인할 수 있습니다. UserName.fit_transform(UserData) #transform, inverse_transformtransform은 앞서 인코딩한 방식에 새로운 데이터셋을 집어넣어 같은 방식으로 변환을 해줍니다. 이때 여러 새로운 피쳐를 집어넣어도 모두 0으로 이루어진 컬럼으로 값을 반환합니다. 원래 데이터가 궁금하다면 inverse_transform을 해주면 됩니다. 새로운 데이터셋에 해당하는 값들은 NaN으로 반환합니다.#BaseN","link":"/2021/05/03/20210503/"},{"title":"","text":"Workflow goals 문제를 해결하는 과정에는 7가지의 핵심 목표가 있습니다. 1. Classifying : 데이터에 대한 분류입니다. 상관관계를 알기위해서 또는 필요에 따라서 특성을 분류해야할 수 있습니다. 2. Correlating : 좋은 결과를 만들기 위해 각 특성의 상관관계를 추측하거나, 검증해야 하는 경우가 많습니다. 아래 단계들을 이용하여 특성과 정답 간의 상 관관계를 알아내야 합니다. 3. Converting : 일부 특성의 경우에는 범주형을 수치형으로 바꾸는 등 변환이 필요합니다. 또한 대부분의 머신러닝 알고리즘은 문자열로 이루어진 범주형 을 입력으로 받을 수 없으니, 대부분의 문제에서 필수적인 과정입니다. (원 핫 인코딩, get dummy함수 사용) 4. Completing : 일부 손실된 데이터는 채워줘야 할 필요가 있습니다. 빈 데이터는 알고리즘에 있어 이상치로 존재하며 알고리즘의 성능을 낮출 수 있습니다. (결측치 처리) 5. Correcting : 데이터도 경우에 따라 오류가 있거나, 노이즈가 섞일 가능성이 있습니다. 이런 경우 일부는 직접 수정해야 하는 경우가 생깁니다. 6. Creating : 특성을 통해 새로운 특성을 만들수도 있습니다. 7. Charting : 그래프와 차트를 통해 결과를 위한 과정을 더 직관적으로 볼 수 있습니다. 시각화를 통해 얻은 직관으로 결과에 필요한 작업을 알 수 있습니다. EDA - 수치형은 수치형대로, 범주형은 범주형대로, 서수형은 서수형대로 특성들을 하나하나 살펴보는 것 1.EDA란? 정의수집한 데이터가 들어왔을 때, 이를 다양한 각도에서 관찰하고 이해하는 과정입니다. 한마디로 데이터를 분석하기 전에 그래프나 통계적인 방법으로 자료를 직관적으로 바라보는 과정입니다. 필요한 이유데이터의 분포 및 값을 검토함으로써 데이터가 표현하는 현상을 더 잘 이해하고, 데이터에 대한 잠재적인 문제를 발견할 수 있습니다. 이를 통해, 본격적인 분석에 들어가기에 앞서 데이터의 수집을 결정할 수 있습니다. 다양한 각도에서 살펴보는 과정을 통해 문제 정의 단계에서 미쳐 발생하지 못했을 다양한 패턴을 발견하고, 이를 바탕으로 기존의 가설을 수정하거나 새로운 가설을 세울 수 있습니다. 과정기본적인 출발점은 문제 정의 단계에서 세웠던 연구 질문과 가설을 바탕으로 분석 계획을 세우는 것입니다. 분석 계획에는 어떤 속성 및 속성 간의 관계를 집중적으로 관찰해야 할지, 이를 위한 최적의 방법은 무엇인지가 포함되어야 합니다. 분석의 목적과 변수가 무엇이 있는지 확인. 개별 변수의 이름이나 설명을 가지는지 확인 데이터를 전체적으로 살펴보기 : 데이터에 문제가 없는지 확인. head나 tail부분을 확인, 추가적으로 다양한 탐색(이상치, 결측치 등을 확인하는 과정) 데이터의 개별 속성값을 관찰 : 각 속성 값이 예측한 범위와 분포를 갖는지 확인. 만약 그렇지 않다면, 이유가 무엇인지를 확인. 속성 간의 관계에 초점을 맞추어, 개별 속성 관찰에서 찾아내지 못했던 패턴을 발견 (상관관계, 시각화 등) 이상값을 찾아내는 부분데이터에 이상치가 있으면, 이상치가 왜 발생했는지 의미를 파악하는 것이 중요합니다. 그리고 그러한 의미를 파악했으면 어떻게 대처해야 할지(제거, 대체, 유지 등)를 판단해야 합니다. 이상치를 발견하는 기법은 여러 가지가 있고 대표적으로 아래와 같은 방법들이 있습니다. 개별 데이터 관찰 데이터값을 눈으로 쭉 훑어보면서 전체적인 추세와 특이사항을 관찰할 수 있습니다. 데이터가 많다고 앞부분만 보면 안 되고, 인덱스에 따른 패턴이 나타날 수도 있으므로 앞, 뒤 or 무작위로 표본을 추출해서 관찰해야 합니다. 단, 이상치들은 작은 크기의 표본에 나타나지 않을 수 있습니다. 통계 값 활용 적절한 요약 통계 지표(summary statistics)를 사용할 수 있습니다. 데이터의 중심을 알기 위해서는 평균(mean), 중앙값(median), 최빈값(mode)을 사용할 수 있고 데이터의 분산을 알기 위해 범위(range), 분산(variance)을 사용할 수 있습니다. 통계 지표를 이용할 때는 데이터의 특성에 주의해야 합니다. 예를 들어, 평균에는 집합 내 모든 데이터 값이 반영되기 때문에, 이상치가 있으면 값이 영향을 받지만, 중앙값에는 가운데 위치한 값 하나가 사용되기 때문에 이상치의 존재에도 대표성이 있는 결과를 얻을 수 있습니다. 회사 직원들의 연봉에 대해서 평균을 구하면, 대개 중간값보다 훨씬 높게 나오는데, 그것은 몇몇 고액 연봉자가 평균을 끌어올렸기 때문입니다. 시각화 활용 일단은 시각적으로 표현이 되어있는 것을 보면, 분석에 도움이 많이 됩니다. 시각화를 통해 주어진 데이터의 개별 속성에 어떤 통계 지표가 적절한지 결정할 수 있습니다. 시각화 방법에는 확률밀도 함수, 히스토그램, 점 플롯(dotplot), 워드 클라우드, 시계열 차트, 지도 등이 있습니다.","link":"/2021/04/27/20210427/"},{"title":"","text":"RNN/ RNN은 히든 노드가 방향을 가진 엣지로 연결돼 순환구조를 이루는 인공신경망의 한 종류다. 음성, 문자 등 순차적으로 등장하는 데이터 처리에 적합한 모델로 알려져 있다. 시계열 데이터 - 시간의 영향을 받는 데이터 Ex) 지금 비가 많이 오고 있으니 10분뒤 날씨 예측 가능 이런 순차 데이터는 기존 forward neural network로 처리하기 힘들기 때문에 RNN이 생겼다 RNN로 인해 sequential data를 분류하는 모델을 만들수 있으나 *vanishing gradient 문제가 발생하게 된다 *Backpropagation이 진행될수록 미분값이 0 에 가까워지는 현상이 발생 문장이 길어질수록 RNN은 쓰기 힘듬 이 문제를 해결 하기 위해 LSTM 이 나옴","link":"/2021/05/28/20210528/"},{"title":"c++","text":"vim 또는 다른 편집기로 test.md 열기 test.md 파일에 다음 코드를 추가합니다.","link":"/2021/03/22/20210608/"}],"tags":[{"name":"python","slug":"python","link":"/tags/python/"},{"name":"R","slug":"R","link":"/tags/R/"},{"name":"c++","slug":"c","link":"/tags/c/"},{"name":"Python","slug":"Python","link":"/tags/Python/"},{"name":"ppt","slug":"ppt","link":"/tags/ppt/"}],"categories":[]}